{
    "issues": [
        {
            "title": "Support parenthesized expression in filter",
            "description": "### Description\r\nParenthesized expression cannot work as expected in filter:\r\n`source = $testTable | where (country = 'Canada' or age > 60) and age < 25 | fields name, age, country` failed in parser.\r\n\r\n### Related Issues\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/887\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [ ] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": []
        },
        {
            "title": "[BUG] Parenthesized expression doesn't work in filter",
            "description": "**What is the bug?**\r\n ```\r\nsource = $testTable | where (country = 'Canada' or age > 60) and age < 25 | fields name, age, country\r\n```\r\nFailed in parser.\r\n",
            "comments": []
        },
        {
            "title": "[RFC] Accelerating Spark Workloads by With Materialized Views",
            "description": "**Problem Overview**\r\nIn modern database engines, query optimization is key to delivering fast, efficient results. Traditional approaches, such as rule-based or cost-based optimizers and distributed execution engines, work to identify the most efficient execution plan. However, these methods have limitations, particularly in complex, high-volume workloads. To address this, we propose an alternative solution: accelerating query execution through materialized views, leveraging a time-space tradeoff to improve performance.\r\n\r\n**Proposed Solution**\r\nOur approach centers around an architecture that introduces a feedback loop between query plan telemetry and workload-driven optimizations. The solution uses materialized views to precompute and store intermediate query results, which can be reused by future queries to reduce execution time. This methodology can be applied in multiple database contexts, including OpenSearch and MPP/Data Lake engines like Spark, Presto, and Trino.\r\n\r\n**Key Components of the Architecture:**\r\n1. Workload:\r\nCollect telemetry data, such as execution plans, cardinality, runtime, and memory usage during query execution. These metrics will feed into the view selection process.\r\n2. Telemetry Preprocessing:\r\nTransform collected telemetry into a uniform representation to standardize data across and optimize the view selection process.\r\n3. Heuristic-Based View Selection:\r\nAnalyze the workload and automatically identify sub-queries that benefit from materialization. We propose a simple heuristic algorithm that selects views based on estimated performance gains and materialization costs\r\n4. Materialized View Creation and Maintenance:\r\nMaterialized views are created and maintained to ensure data consistency. These views store intermediate results (e.g., summary aggregates, sub-query results) that can be reused to speed up query execution.\r\nImplementation Plan:\r\n\r\n**Focus on VPC Flow Logs**: We plan to use VPC flow logs as test data, inserting them into an AWS OpenSearch cluster and generate workloads to test the architecture. We'll collect telemetry by listening to plan execution and gather metrics on cardinality, time, and memory usage. This data will be used to create a workload representation.\r\n\r\n**Heuristic-Based View Selection**: We aim to implement a simple, heuristic-based algorithm for view selection. This algorithm will automatically select views that offer significant performance boosts with minimal materialization costs. By focusing on simplicity, we can ensure rapid implementation and real-time optimization during query execution.\r\n",
            "comments": []
        },
        {
            "title": "Apply shaded rules",
            "description": "### Description\r\n\r\nApply shaded rules to prevent potential jar hell issue \r\n\r\n### Check List\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": []
        },
        {
            "title": "Ppl count approximate support",
            "description": "### Description\r\nsupport approximation operations for \r\n - count distinct\r\n - top\r\n - rare\r\n \r\n### Related Issues\r\n#882 \r\n\r\n### related context\r\n- https://spark.apache.org/docs/3.5.2/sql-ref-functions-builtin.html\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [X] Implemented tests for combination with other commands\r\n- [X] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": []
        },
        {
            "title": "Fix bug for not able to get sourceTables from metadata",
            "description": "### Description\r\nUsing custom implementation for `FlintIndexMetadataService` exposes a bug where we incorrectly get empty `sourceTables` from `FlintMetadata`.\r\nDepending on how the `FlintMetadata` is generated, the `metadata.properties.get(\"sourceTables\")` could either be of type java array or scala array.\r\nThis PR fixes the bug, now considering both cases.\r\n\r\nAlso added some logs for troubleshooting in production environment.\r\n\r\n\r\n### Related Issues\r\n* #746 \r\n* #854 \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": []
        },
        {
            "title": "[FEATURE]PPL Support approximation count for improved performance",
            "description": "**Is your feature request related to a problem?**\r\nIn large volume datasets and/or high cardinality datasets the counting is an expensive computation.\r\nSpark has a list of build-in approximation function such as:\r\n - [approx_count_distinct](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_count_distinct.html) which returns the estimated number of distinct values in expr within the group.\r\n     - ```sql\r\n         SELECT approx_count_distinct(column_name) FROM table_name;\r\n       ```\r\n     \r\n- [approx_percentile](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_percentile.html) which returns the approximate percentile of the numeric column col which is the smallest value in the ordered col values \r\n     - ```sql\r\n         SELECT \r\n            col_name,\r\n                approx_percentile(col_name, 0.5) AS median_value,\r\n                approx_percentile(col_name, array(0.25, 0.5, 0.75)) AS quartiles\r\n            FROM \r\n                table_name\r\n            GROUP BY \r\n                col_name\r\n       ```\r\n\r\n**What solution would you like?**\r\nWe would like to add these approximation capabilities to every PPL function that can offer counting:\r\n\r\n- `Top` & `Rare` commands\r\n```sql\r\n... | top_approx 5 values by country \r\n... | rare_approx age by gender\r\n```\r\n\r\n- `Stats`\r\n```sql\r\n... | stats count_distinct_approx(c) by b | head 5` \r\n```\r\n\r\n**Do you have any additional context?**\r\n- https://spark.apache.org/docs/3.5.2/sql-ref-functions-builtin.html",
            "comments": []
        },
        {
            "title": "[FEATURE]PPL/SQL Geo Functionality Support",
            "description": "**Is your feature request related to a problem?**\r\nWe would like to add advanced geo-spatial commands support.\r\n\r\n### Context\r\nHere\u2019s a list of common SQL/PPL geo-spatial commands, often used in databases like PostGIS, Oracle Spatial, and other geospatial extensions for SQL-compliant systems.\r\n\r\nThese can be implemented or adapted in PPL / SQL (under the Spark Engine) for spatial analysis tasks.\r\n\r\n### Geospatial Functions for Geometry Operations\r\n#### Distance: Computes the distance between two geometries (e.g., points, polygons).\r\n`Distance(geomA, geomB)`\r\n\r\n#### Intersects: Determines if two geometries intersect.\r\n`Intersects(geomA, geomB)`\r\n\r\n#### Contains: Checks if one geometry contains another.\r\n`Contains(geomA, geomB)`\r\n\r\n#### Within: Determines if one geometry is within another.\r\n`Within(geomA, geomB)`\r\n\r\n#### Overlaps: Checks if two geometries overlap.\r\n`Overlaps(geomA, geomB)`\r\n\r\n#### Crosses: Tests if one geometry crosses another.\r\n`Crosses(geomA, geomB)`\r\n\r\n### Geospatial Transformation Functions\r\n\r\n#### Transform: Transforms a geometry to a different spatial reference system (SRID).\r\n`Transform(geometry, targetSRID)`\r\n\r\n#### Buffer: Creates a buffer area around a geometry at a specified distance.\r\n`Buffer(geometry, distance)`\r\n\r\n#### Centroid: Calculates the centroid (center point) of a geometry.\r\n`Centroid(geometry)`\r\n\r\n#### Union: Merges multiple geometries into one.\r\n`Union(geomA, geomB)`\r\n\r\n#### Difference: Computes the difference between two geometries.\r\n`Difference(geomA, geomB)`\r\n\r\n#### Simplify: Reduces the complexity of a geometry, keeping its shape intact within a specified tolerance.\r\n`Simplify(geometry, tolerance)`\r\n\r\n### Geometry Creation and Manipulation\r\n\r\n#### Point: Creates a point geometry from given X and Y coordinates.\r\n`Point(x, y)`\r\n\r\n#### LineString: Creates a line geometry from an array of points.\r\n`LineString([point1, point2, ...])`\r\n\r\n#### Polygon: Forms a polygon geometry from boundary coordinates.\r\n`Polygon(boundary)`\r\n\r\n#### MakeEnvelope: Creates a rectangular polygon based on corner coordinates.\r\n`MakeEnvelope(xMin, yMin, xMax, yMax, SRID)`\r\n\r\n### Spatial Index and Aggregate Functions\r\n\r\n#### GeomFromText: Converts Well-Known Text (WKT) representation to a geometry.\r\n`GeomFromText('POINT(1 1)')`\r\n\r\n### AsText: Converts a geometry to its WKT representation.\r\n`AsText(geometry)`\r\n\r\n#### Area: Computes the area of a polygonal geometry.\r\n`Area(geometry)`\r\n\r\n#### Length: Calculates the length of a linear geometry.\r\n`Length(geometry)`\r\n\r\n#### Collect: Aggregates multiple geometries into a single multi-geometry.\r\n`Collect(geometry_set)`\r\n\r\n---\r\n`geometry` refers to spatial data types used to represent shapes, locations, and spatial relationships. This typically includes various geometric objects such as:\r\n\r\n- **Point**: Represents a single location in space, defined by X (longitude) and Y (latitude) coordinates.\r\n- **LineString**: Represents a sequence of points that form a continuous line, such as roads, rivers, or paths.\r\n- **Polygon**: A closed shape, defined by a series of points that create a boundary, often used to represent areas like countries, lakes, or other closed regions.\r\n\r\n- **MultiPoint, MultiLineString, MultiPolygon:** Collections of points, lines, or polygons, respectively, allowing multiple geometries to be treated as one.\r\n- **GeometryCollection**: A collection of geometries of different types, such as a mix of points, lines, and polygons.\r\n\r\nThese commands cover a broad range of spatial operations used for queries, analysis, and transformations in SQL environments\r\n\r\n**Do you have any additional context?**\r\n- https://github.com/apache/sedona",
            "comments": []
        },
        {
            "title": "Add `sample` parameter to `top` & `rare` command",
            "description": "### Description\r\nAdd a new sample command (`sample`) to reduce amount of scanned data points and allow approximation of a `top` or `rare` statements when faster sample based results if favour of exact long running results \r\n\r\n```sql\r\nsource = testTable  | rare address sample(50 percent)\r\nsource = testTable  | top 5 address by country sample(25 percent)\r\n```\r\n\r\n### Issues Resolved\r\n- https://github.com/opensearch-project/opensearch-spark/issues/740\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "LantaoJin",
                    "comment": "One high level question:\r\nHow do we determine the relationship between percentage and precision? Or how much precision does it lose when sampling is decreased from 100% to 80% or from 80% to 50%?\r\n\r\nI'm wondering what kind of scenario needs to run `top` on the sample data."
                }
            ]
        },
        {
            "title": "Add sanity test script",
            "description": "### Description\r\nThis Python script executes test queries from a CSV file using an asynchronous query API and generates comprehensive test reports.\r\n\r\nThe script produces two report types:\r\n1. An Excel report with detailed test information for each query\r\n2. A JSON report containing both test result overview and query-specific details\r\n\r\nApart from the basic feature, it also has some advanced functionality includes:\r\n1. Concurrent query execution (note: the async query service has session limits, so use thread workers moderately despite it already supports session ID reuse)\r\n2. Configurable query timeout with periodic status checks and automatic cancellation if timeout occurs.\r\n3. Flexible row selection from the input CSV file, by specifying start row and end row of the input CSV file.\r\n4. Expected status validation when expected_status is present in the CSV\r\n5. Ability to generate partial reports if testing is interrupted\r\nAn example to run the test script:\r\n```\r\npython SanityTest.py --base-url ${URL_ADDRESS} --username *** --password *** --datasource ${DATASOURCE_NAME} --input-csv test_queries.csv --output-file test_report --max-workers 2 --check-interval 10 --timeout 600\r\n```\r\nFor more details, you can see the help manual via command:\r\n```\r\npython SanityTest.py --help   \r\n\r\nusage: SanityTest.py [-h] --base-url BASE_URL --username USERNAME --password PASSWORD --datasource DATASOURCE --input-csv INPUT_CSV\r\n                                      --output-file OUTPUT_FILE [--max-workers MAX_WORKERS] [--check-interval CHECK_INTERVAL] [--timeout TIMEOUT]\r\n                                      [--start-row START_ROW] [--end-row END_ROW]\r\n\r\nRun tests from a CSV file and generate a report.\r\n\r\noptions:\r\n  -h, --help            show this help message and exit\r\n  --base-url BASE_URL   Base URL of the service\r\n  --username USERNAME   Username for authentication\r\n  --password PASSWORD   Password for authentication\r\n  --datasource DATASOURCE\r\n                        Datasource name\r\n  --input-csv INPUT_CSV\r\n                        Path to the CSV file containing test queries\r\n  --output-file OUTPUT_FILE\r\n                        Path to the output report file\r\n  --max-workers MAX_WORKERS\r\n                        optional, Maximum number of worker threads (default: 2)\r\n  --check-interval CHECK_INTERVAL\r\n                        optional, Check interval in seconds (default: 10)\r\n  --timeout TIMEOUT     optional, Timeout in seconds (default: 600)\r\n  --start-row START_ROW\r\n                        optionl, The start row of the query to run, start from 0\r\n  --end-row END_ROW     optional, The end row of the query to run\r\n```\r\n\r\n### Related Issues\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/877\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [ ] Implemented unit tests\r\n- [ ] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@qianheng-aws this looks good - please add\r\n- documentation with all the above description and instructions + results report example\r\n- what are the prerequisites for running the script - docker ? \r\n- what are the data prerequisites for running the script - import content process ?\r\n- export the results to a file including :\r\n   -  ppl version\r\n   -  performance time \r\n   -  coverage\r\n - plz also add a folder for CSV tests   "
                },
                {
                    "user": "qianheng-aws",
                    "comment": "> @qianheng-aws this looks good - please add\r\n> \r\n> * documentation with all the above description and instructions + results report example\r\n> * what are the prerequisites for running the script - docker ?\r\n> * what are the data prerequisites for running the script - import content process ?\r\n> * export the results to a file including :\r\n>   \r\n>   * ppl version\r\n>   * performance time\r\n>   * coverage\r\n> * plz also add a folder for CSV tests\r\n\r\nI addressed most part  of the comment and add them in the README.md, except some points:\r\n- the data prerequisites. I haven't got the background of where the data comes from and how to ingest them to flint yet. Left a TODO in the README.\r\n- ppl version. Do you mean the version returns from API `GET _cat/plugins?v`?  I think it should always be the same as the opensearch version. Will it help to add that in the report?\r\n"
                }
            ]
        },
        {
            "title": "[FEATURE] Support running sanity test automatically",
            "description": "**Is your feature request related to a problem?**\r\nCurrently, we need to run sanity test one by one manually in the OpenSearch dashboard. It's inefficient and fussy to get a final test report overall test results.\r\n\r\n**What solution would you like?**\r\nNeed to develop a script to run theses tests automatically and generate a final report itself.\r\n\r\n**What alternatives have you considered?**\r\n\r\n**Do you have any additional context?**\r\n",
            "comments": []
        },
        {
            "title": "[BUG] Query cannot run in active session as long as the concurrent active session reach to 10",
            "description": "**What is the bug?**\r\nGot follow error message when submit query in an active session:\r\n{ \"status\": 429, \"error\": { \"type\": \"ConcurrencyLimitExceededException\", \"reason\": \"Too Many Requests\", \"details\": \"domain concurrent active session can not exceed 10\" } }\r\n\r\nThis is a bug to fix, since we should check the limit in create session instead of submitting query in active session.\r\n",
            "comments": [
                {
                    "user": "noCharger",
                    "comment": "This is expected behavior. Please ref\nhttps://github.com/opensearch-project/sql/pull/2390 for details "
                },
                {
                    "user": "LantaoJin",
                    "comment": "@noCharger no. I don't think so. The limitation implementation in my option is not correct. The limit of total concurrent active sessions should only impact the 11th session. But currently, when there are 10 concurrent active sessions, all the 10 active sessions won't work as expected. For example, you are opening a session and submit queries in this only active session you have. Now someone else create 9 sessions. You will find your active active session (actually all 10 active sessions) become \"inactive\" suddenly. "
                },
                {
                    "user": "ykmr1224",
                    "comment": "@LantaoJin +1. So the `leaseManager.borrow` should be called when creating new session."
                }
            ]
        },
        {
            "title": "[BUG] Flint Query cannot read DATE data type",
            "description": "**What is the bug?**\r\nThe generated TPCH table files have uploaded to S3. For example, create a parquet table with extended S3 location:\r\n```sql\r\nUSE tpch_sf_1;\r\nCREATE TABLE `lineitem` (`l_orderkey` LONG, `l_partkey` LONG, `l_suppkey` LONG,\r\n`l_linenumber` LONG, `l_quantity` LONG, `l_extendedprice` LONG,\r\n`l_discount` LONG, `l_tax` LONG, `l_returnflag` STRING,\r\n`l_linestatus` STRING, `l_shipdate` DATE, `l_commitdate` DATE, `l_receiptdate` DATE,\r\n`l_shipinstruct` STRING, `l_shipmode` STRING, `l_comment` STRING)\r\nUSING parquet\r\nLOCATION 's3://do-not-delete-flint-integ-tests-us-east-1/data/tpch/sf=1/lineitem/';\r\n```\r\n\r\nQuery in Flint failed when access the DATE type field:\r\n```sql\r\nSELECT l_shipdate FROM myglue_test.tpch_sf_1.lineitem LIMIT 10;\r\n```\r\nor\r\n```sql\r\nsource = myglue_test.tpch_sf_1.lineitem | head 10 | fields l_shipdate\r\n```\r\nIn QueryBench, it shows `Failed to query status`. But no problem to access other datatype fields such as `| fields l_returnflag`. \r\n\r\nThe failed query works well in Spark local as follow:\r\n```sql\r\nCREATE TABLE `lineitem` (`l_orderkey` LONG, `l_partkey` LONG, `l_suppkey` LONG,\r\n`l_linenumber` LONG, `l_quantity` LONG, `l_extendedprice` LONG,\r\n`l_discount` LONG, `l_tax` LONG, `l_returnflag` STRING,\r\n`l_linestatus` STRING, `l_shipdate` DATE, `l_commitdate` DATE, `l_receiptdate` DATE,\r\n`l_shipinstruct` STRING, `l_shipmode` STRING, `l_comment` STRING)\r\nUSING parquet\r\nLOCATION 'file:///Users/ltjin/Downloads/tpch/data/tpch/sf=1/lineitem/';\r\n\r\nSELECT l_shipdate FROM lineitem LIMIT 10;\r\n```\r\n\r\n**How to reproduce?**\r\nBelow simple query failed in Query Workbench -> SQL\r\n```sql\r\nselect date('2010-02-11')\r\n```",
            "comments": []
        },
        {
            "title": "[FEATURE] Extra options should offer a more explicit method for users to specify the table identifier.",
            "description": "**Is your feature request related to a problem?**\r\nIf the CREATE MATERIALIZED VIEW statement accesses the db.default.tbl-001 table and the user wants to pass stat_timestamp as an option, the table name must be enclosed in backticks in extra_options. Without backticks, the options cannot be passed correctly.\r\nThe root cause is that when UnresolvedRelation.getTableName is called, any identifiers containing special characters are automatically [quoted](https://github.com/apache/spark/blob/branch-3.5/sql/api/src/main/scala/org/apache/spark/sql/catalyst/util/QuotingUtils.scala#L44C1-L50C4).\r\n```\r\nCREATE MATERIALIZED VIEW `db`.`default`.`mv` AS SELECT * FROM `db`.`default`.`tbl-001` WITH ( auto_refresh = true, refresh_interval = '5 Minute', extra_options = '{\"db.default.`tbl-001`\": {\"start_timestamp\": \"1729493689000\"}}'\r\n```\r\n\r\n**What solution would you like?**\r\nA more explicit method is needed for users to specify the table identifier. The expected behavior is that whatever the user specifies in the FROM clause should be used directly in extra_options. For example, in the case above, the user should specify the identifier as written in the FROM clause.\r\n```\r\nextra_options = '{\"`db`.`default`.`tbl-001`\": {\"start_timestamp\": \"1729493689000\"}\r\n```\r\n\r\n**What alternatives have you considered?**\r\nn/a\r\n\r\n**Do you have any additional context?**\r\nAdd any other context or screenshots about the feature request here.",
            "comments": []
        },
        {
            "title": "New trendline ppl command (WMA)",
            "description": "### Description\r\nIntroduce a new variant (WMA) for existing trendline ppl command, by compositing a logical plan similar to the following with function `nth_value( )` to calculate the WMA value by perform event look behind. \r\n\r\n```\r\n-- +- 'Project ['name, 'salary, \r\n-- (((('nth_value('salary, 3) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 3) + \r\n-- ('nth_value('salary, 2) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 2)) + \r\n-- ('nth_value('salary, 1) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 1)) / 6) AS WMA#708]\r\n   -- +- 'UnresolvedRelation [employees], [], false\r\n```\r\n\r\n\r\nSome high level code changes:\r\n - Update developer README to include selected set of Unit || Integration test\r\n - Update example for WMA command usage\r\n - Update CatelystQueryPlanVisotor related classes to provide sort option argument into `TrendLine` processing logic, as sort field is mandatory for WMA calculation.\r\n - Update `TrendlineCatalystUtils.java` to have a new code path for WMA selection and associated calculation logic.\r\n\r\n\r\n### Related Issues\r\nPrior implement for SMA formula: https://github.com/opensearch-project/opensearch-spark/pull/833\r\n\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\r\n### Test plan:\r\nDespite the existing unit test / integration test, the feature can also be tested manually, by first inserting a simple table, then run PPL trend line command against the table to calculate WMA value.\r\n```\r\n# Produce the artifact\r\nsbt clean sparkPPLCosmetic/publishM2\r\n\r\n# Start Spark with the plugin\r\nbin/spark-sql --jars \"/ABSOLUTE_PATH_TO_ARTIFACT/opensearch-spark-ppl_2.12-0.6.0-SNAPSHOT.jar\" \\\r\n--conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintPPLSparkExtensions\"  \\\r\n--conf \"spark.sql.catalog.dev=org.apache.spark.opensearch.catalog.OpenSearchCatalog\" \\\r\n--conf \"spark.hadoop.hive.cli.print.header=true\"\r\n\r\n# Insert test table and data\r\nCREATE TABLE employees (name STRING, dept STRING, salary INT, age INT, con STRING);\r\n\r\nINSERT INTO employees VALUES (\"Lisa\", \"Sales------\", 10000, 35, 'test');\r\nINSERT INTO employees VALUES (\"Evan\", \"Sales------\", 32000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Fred\", \"Engineering\", 21000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Alex\", \"Sales\", 30000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Tom\", \"Engineering\", 23000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Jane\", \"Marketing\", 29000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Jeff\", \"Marketing\", 35000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Paul\", \"Engineering\", 29000, 23, 'test');\r\nINSERT INTO employees VALUES (\"Chloe\", \"Engineering\", 23000, 25, 'test');\r\n\r\n# Execute WMA with basic option:\r\n\r\nsource=employees | trendline sort age wma(2, salary);\r\n\r\nname\tdept\tsalary\tage\tcon\tsalary_trendline\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\r\n\r\n\r\n# Execute WMA with alias:\r\n\r\nsource=employees | trendline sort age wma(2, salary) as CUSTOM_NAME\r\n\r\nname\tdept\tsalary\tage\tcon\tCUSTOM_NAME\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\r\n\r\n\r\n# Execute WMA with multiple calculations:\r\n\r\nsource=employees | trendline sort age wma(2, salary) as WMA_2 wma(3, salary) as WMA_3;\r\n\r\n\r\nname\tdept\tsalary\tage\tcon\tWMA_2\tWMA_3\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\tNULL\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\t24000.0\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\t26833.333333333332\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\t25000.0\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\t17666.666666666668\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\t24666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\t29333.333333333332\r\nTime taken: 0.466 seconds, Fetched 9 row(s)\r\n\r\n\r\n```\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@andy-k-improving please add DCO (sign-off)"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "> @andy-k-improving please add DCO (sign-off)\r\n\r\nDone."
                },
                {
                    "user": "andy-k-improving",
                    "comment": "> @andy-k-improving\r\n> \r\n> ## please add the relevant [documentation](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/ppl-lang/ppl-trendline-command.md) references including [examples](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/ppl-lang/PPL-Example-Commands.md)\r\n\r\nDone."
                },
                {
                    "user": "andy-k-improving",
                    "comment": "@YANG-DB I have updated the example and documentation, would you mind to have another look?\r\n\r\nThanks, "
                }
            ]
        },
        {
            "title": "[WIP] PPL geoip function pt. 2",
            "description": "Description\r\nPPL geoip function\r\n\r\nIssues Resolved\r\nhttps://github.com/opensearch-project/opensearch-spark/issues/672\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\r\nThis PR is a continuation of https://github.com/opensearch-project/opensearch-spark/pull/781 due to lacking permissions to push to forked branch in said PR\r\n",
            "comments": [
                {
                    "user": "kenrickyap",
                    "comment": "Hi @YANG-DB, I heard from Anas that you had a method of implementing ip2geo functionality for Spark. I wanted to check with you that our current approach aligns with your method.\r\n\r\nCurrent Plan:\r\nLeveraging [SerializableUdf](https://github.com/opensearch-project/opensearch-spark/pull/706/files#diff-4fc2e6dc7264b8eac89050f1f19c51381f61fa36b302d78bcc0bc260498845fe) create a UDF that does the follow:\r\n\r\n1. Check if in-memory cache object for datasource exists.\r\n2. If cache object does not exists create new in-memory cache object from csv retrieved from datasource manifest. (manifest to CsvParser logic can be stripped from geospatial ip2geo).\r\n4. Search cached object for GeoIP data.\r\n5. Return GeoIP data.\r\n\r\nThis PR has a stub udf implementation for better idea of how this would be implemented.\r\n\r\nAll of this would have to be implemented within the Spark library, as currently I am not aware of how to access any geospatial artifacts. If you know of a better way to implement ip2geo please let me know! Thanks!"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@kenrickyap please update the DCO (contributor sign-off) \r\n"
                },
                {
                    "user": "kenrickyap",
                    "comment": "**Flow diagram for ip2geo UDF**\r\n\r\n![mermaid-diagram-2024-11-06-140042](https://github.com/user-attachments/assets/a7d48707-0348-4558-9b45-8a51a353d1d8)\r\n\r\n**Design Details**\r\n\r\n- Ip2GeoCache will be a Gauva Cache that has datasource string as key and CidrTree as value\r\n- CidrTree will be Trie (will use apache.common PatriciaTrie as I see that apache.common is already included in project)\r\n  - Am using trie instead of map as it is more well suited for longest prefix matching task, and this task is similar to ip to cidr matching\r\n  - each CidrTreeNode will store:\r\n    - nth bit value of cidr (n is the depth of tree)\r\n    - geo_data if there is matching cidr row in datasource csv\r\n    - child CidrTreeNodes\r\n - Will retrieve CsvParser from manifest using similar methodology as in geospatial [ip2geo](https://github.com/opensearch-project/geospatial/blob/main/src/main/java/org/opensearch/geospatial/ip2geo/dao/GeoIpDataDao.java#L188)\r\n \r\n **Pros**\r\n \r\n - ip2geo functionality is achieved.\r\n - implementation is simple and does not depend on any additional libraries that do not already exist in the project.\r\n \r\n **Cons**\r\n \r\n - calculations are done in-memory as a UDF, this means that multiple instances of Ip2GeoCache will be created in distributed Spark systems and they will not sync.\r\n - not leveraging job-scheduler to run ip2geo task and not leveraging OpenSearch to store ip2geo data as geospatial does."
                },
                {
                    "user": "YANG-DB",
                    "comment": "> **Flow diagram for ip2geo UDF**\r\n> \r\n> ![mermaid-diagram-2024-11-06-140042](https://private-user-images.githubusercontent.com/121634635/383740205-a7d48707-0348-4558-9b45-8a51a353d1d8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA5MzcxNTksIm5iZiI6MTczMDkzNjg1OSwicGF0aCI6Ii8xMjE2MzQ2MzUvMzgzNzQwMjA1LWE3ZDQ4NzA3LTAzNDgtNDU1OC05YjQ1LThhNTFhMzUzZDFkOC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwNlQyMzQ3MzlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03ZGE4ZTM3NzE5NDU1Y2NlOWNhNjVhNDFjOWFiZDBiZWJjZWIzOWRkYzNmMDllMmM5MmFhY2Y0ZDJmNzZkNjI1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.xQmnmdwvtN2M2Dt0INzk-OtazcQRuErwn7ewiaKvRGg)\r\n> \r\n> **Design Details**\r\n> \r\n> * Ip2GeoCache will be a Gauva Cache that has datasource string as key and CidrTree as value\r\n> * CidrTree will be Trie (will use apache.common PatriciaTrie as I see that apache.common is already included in project)\r\n>   \r\n>   * Am using trie instead of map as it is more well suited for longest prefix matching task, and this task is similar to ip to cidr matching\r\n>   * each CidrTreeNode will store:\r\n>     \r\n>     * nth bit value of cidr (n is the depth of tree)\r\n>     * geo_data if there is matching cidr row in datasource csv\r\n>     * child CidrTreeNodes\r\n> * Will retrieve CsvParser from manifest using similar methodology as in geospatial [ip2geo](https://github.com/opensearch-project/geospatial/blob/main/src/main/java/org/opensearch/geospatial/ip2geo/dao/GeoIpDataDao.java#L188)\r\n> \r\n> **Pros**\r\n> \r\n> * ip2geo functionality is achieved.\r\n> * implementation is simple and does not depend on any additional libraries that do not already exist in the project.\r\n> \r\n> **Cons**\r\n> \r\n> * calculations are done in-memory as a UDF, this means that multiple instances of Ip2GeoCache will be created in distributed Spark systems and they will not sync.\r\n> * not leveraging job-scheduler to run ip2geo task and not leveraging OpenSearch to store ip2geo data as geospatial does.\r\n\r\n@kenrickyap a few questions here:\r\n- how / is the Ip2GeoCache memory shared between sessions ? is it a signleton ? \r\n- plz also add the use case where the geo_data_source will be a table / index (in OpenSearch) / service (API) - lets create a general purpose facade for this to hide different datasource drivers\r\n- I would like to see a more detailed description of the `CidrTreeNode` including an example and simple high level explanations (the geo tree diagram ?)\r\n- explain why is it worth while to add this trie-tree instead of using a hash-map / search ?\r\n- can u also add a pseudo-code here (in the issue) to clarity the composition ?\r\n"
                },
                {
                    "user": "kenrickyap",
                    "comment": "> @kenrickyap a few questions here:\r\n> \r\n> * how / is the Ip2GeoCache memory shared between sessions ? is it a signleton ?\r\n\r\nYes we will create Ip2GeoCache to be a singleton. From my understanding this should allow the cache to be accessible between sessions \r\n\r\n> * plz also add the use case where the geo_data_source will be a table / index (in OpenSearch) / service (API) - lets create a general purpose facade for this to hide different datasource drivers\r\n\r\nWill update the flow diagram to reflect facade for different datasource. \r\n\r\nHowever, would it be possible to provide a service (API) example usage? I am not to sure what such a datasource would be expected to return. \r\n\r\nAlso would there be a fixed schema for an index in opensource?\r\n\r\n> * I would like to see a more detailed description of the `CidrTreeNode` including an example and simple high level explanations (the geo tree diagram ?)\r\n\r\nIn hindsight I will just use a hashmap to store cidr geo_data where the cidr bitstring will be the key and geo_data will be a value. To preform the ip cidr matching will implement lookup function to convert ip to bitstring and reduce bit length till key is found in map.\r\n\r\nInitially I wanted to leverage the `prefixMap` function as I thought this would find the best fitting cidr mask for a given ip in O(1), which would mean I wouldn't have to implement my own lookup function. However as I was trying implement this noticed that `prefixMap` takes a prefix and finds all keys that have the prefix, which is the opposite with what I want.\r\n\r\n> * explain why is it worth while to add this trie-tree instead of using a hash-map / search ?\r\n\r\nAs mentioned above will use a hash-map instead of trie.\r\n\r\n> * can u also add a pseudo-code here (in the issue) to clarity the composition ?\r\n\r\nWill have pseudo-code and code added by EOD"
                }
            ]
        },
        {
            "title": "[FEATURE]Reduce IT Test time",
            "description": "**Is your feature request related to a problem?**\r\n\r\nCurrently it takes more than 60 minutes to complete the tests for the [ test & build github workflow ](https://github.com/opensearch-project/opensearch-spark/blob/main/.github/workflows/test-and-build-workflow.yml)\r\n\r\n**What solution would you like?**\r\nWe need to significantly reduce this time for better development responsiveness and fail-fast\r\n\r\n**Do you have any additional context?**\r\n- attempt to parallel the IT tests",
            "comments": []
        },
        {
            "title": "Add metrics for successful/failed Spark index creation",
            "description": "### Description\r\nSuccess and failure metrics for spark index creation -- note that \"failure\" here means an execution failure (e.g. Spark couldn't execute the query), not validation failure (e.g. the index already exists).\r\n\r\nE.g. success for creating a skipping index:\r\n<img width=\"1170\" alt=\"image\" src=\"https://github.com/user-attachments/assets/856d9ecb-7abc-4ecf-9d1f-f713226c660a\">\r\n\r\n### Related Issues\r\nN/A\r\n\r\n### Check List\r\n- [ ] Updated documentation (docs/ppl-lang/README.md)\r\n- [ ] Implemented unit tests\r\n- [ ] Implemented tests for combination with other commands\r\n- [X] New added source code should include a copyright header\r\n- [X] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": []
        },
        {
            "title": "[FEATURE]PPL new `trendline` command ( WMA)",
            "description": "This is the WMA part of the new spark PPL trendline command\r\n\r\nSee https://github.com/opensearch-project/opensearch-spark/issues/655 for details",
            "comments": [
                {
                    "user": "salyh",
                    "comment": "See https://github.com/eliatra/opensearch-spark/pull/7 for an prototype"
                },
                {
                    "user": "salyh",
                    "comment": "Consider https://github.com/opensearch-project/opensearch-spark/pull/833#issuecomment-2448892431 when implementing WMA"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "Hi @salyh @LantaoJin \r\nI'm  picking this task up base on the existing SMA implementation (https://github.com/opensearch-project/opensearch-spark/pull/833), and the provided prototype (https://github.com/eliatra/opensearch-spark/pull/7)\r\n\r\nThanks!"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "The below is my proposed implementation on Spark SQL with `nth_value` aggregation and associated Logical plan.\r\n\r\n```\r\n\r\n\r\nCREATE TABLE employees (name STRING, dept STRING, salary INT, age INT, con STRING);\r\n\r\nINSERT INTO employees VALUES (\"Lisa\", \"Sales\", 10000, 35, 'test');\r\nINSERT INTO employees VALUES (\"Evan\", \"Sales\", 32000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Fred\", \"Engineering\", 21000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Alex\", \"Sales\", 30000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Tom\", \"Engineering\", 23000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Jane\", \"Marketing\", 29000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Jeff\", \"Marketing\", 35000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Paul\", \"Engineering\", 29000, 23, 'test');\r\nINSERT INTO employees VALUES (\"Chloe\", \"Engineering\", 23000, 25, 'test');\r\n\r\n\r\nexplain extended SELECT name, salary, \r\n(nth_value(salary, 3) OVER w *3 + nth_value(salary, 2) OVER w *2 + nth_value(salary, 1) OVER w *1)/6 AS WMA\r\n\r\nFROM employees \r\nWINDOW w AS (ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)\r\nORDER BY age;\r\n\r\n\r\n\r\n-- 'Sort ['age ASC NULLS FIRST], true\r\n-- +- 'WithWindowDefinition [w=windowspecdefinition('con, 'age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$()))]\r\n--    +- 'Project ['name, 'salary, \r\n            -- ((((unresolvedwindowexpression('nth_value('salary, 3), WindowSpecReference(w)) * 3) + \r\n            -- (unresolvedwindowexpression('nth_value('salary, 2), WindowSpecReference(w)) * 2)) + \r\n            -- (unresolvedwindowexpression('nth_value('salary, 1), WindowSpecReference(w)) * 1)) / 6) AS WMA#611]\r\n--       +- 'UnresolvedRelation [employees], [], false\r\n\r\n```\r\n\r\nI have verified the calculation offline, this should produce the result that we are looking for in this ticket and I'm in the process to convert the same logic into code. \r\n\r\n@salyh and @LantaoJin do you guys foresee any potential issue base on above?\r\n\r\n\r\n\r\n"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "> The below is my proposed implementation on Spark SQL with `nth_value` aggregation and associated Logical plan.\r\n> \r\n> ```\r\n> \r\n> \r\n> CREATE TABLE employees (name STRING, dept STRING, salary INT, age INT, con STRING);\r\n> \r\n> INSERT INTO employees VALUES (\"Lisa\", \"Sales\", 10000, 35, 'test');\r\n> INSERT INTO employees VALUES (\"Evan\", \"Sales\", 32000, 38, 'test');\r\n> INSERT INTO employees VALUES (\"Fred\", \"Engineering\", 21000, 28, 'test');\r\n> INSERT INTO employees VALUES (\"Alex\", \"Sales\", 30000, 33, 'test');\r\n> INSERT INTO employees VALUES (\"Tom\", \"Engineering\", 23000, 33, 'test');\r\n> INSERT INTO employees VALUES (\"Jane\", \"Marketing\", 29000, 28, 'test');\r\n> INSERT INTO employees VALUES (\"Jeff\", \"Marketing\", 35000, 38, 'test');\r\n> INSERT INTO employees VALUES (\"Paul\", \"Engineering\", 29000, 23, 'test');\r\n> INSERT INTO employees VALUES (\"Chloe\", \"Engineering\", 23000, 25, 'test');\r\n> \r\n> \r\n> explain extended SELECT name, salary, \r\n> (nth_value(salary, 3) OVER w *3 + nth_value(salary, 2) OVER w *2 + nth_value(salary, 1) OVER w *1)/6 AS WMA\r\n> \r\n> FROM employees \r\n> WINDOW w AS (ROWS BETWEEN 2 PRECEDING AND CURRENT ROW)\r\n> ORDER BY age;\r\n> \r\n> \r\n> \r\n> -- 'Sort ['age ASC NULLS FIRST], true\r\n> -- +- 'WithWindowDefinition [w=windowspecdefinition('con, 'age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$()))]\r\n> --    +- 'Project ['name, 'salary, \r\n>             -- ((((unresolvedwindowexpression('nth_value('salary, 3), WindowSpecReference(w)) * 3) + \r\n>             -- (unresolvedwindowexpression('nth_value('salary, 2), WindowSpecReference(w)) * 2)) + \r\n>             -- (unresolvedwindowexpression('nth_value('salary, 1), WindowSpecReference(w)) * 1)) / 6) AS WMA#611]\r\n> --       +- 'UnresolvedRelation [employees], [], false\r\n> ```\r\n> \r\n> I have verified the calculation offline, this should produce the result that we are looking for in this ticket and I'm in the process to convert the same logic into code.\r\n> \r\n> @salyh and @LantaoJin do you guys foresee any potential issue base on above?\r\n\r\nAlso the function `nth_value( )` require a sort field, as it can't be execute against the natural order of incoming table, hence I'm proposing to have the `[sort <[+|-] sort-field>]` as mandatory for WMA treandline. \r\nWill that cause any problem, in term of breaking change on trend line PPL syntax?"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "PR for this issue: https://github.com/opensearch-project/opensearch-spark/pull/872"
                }
            ]
        },
        {
            "title": "[Enhancement] OpenSearch client level exception handling",
            "description": "Currently, if the OpenSearch client does not throw an OpenSearchException, the status code returns 500. Other exceptions in Amazon OpenSearch (AOS) can be returned, for example, Runtime exception. And these cases should be handled gracefully if there's statu code.",
            "comments": []
        },
        {
            "title": "[BUG] Dangling flint index when external scheduler operation failed",
            "description": "**What is the bug?**\r\nWhen scheduler operation failed. The index can still be created. This is due to the current implementation of optimistic transaction only rollback the log entry without clean up the resources\r\n\r\n**How can one reproduce the bug?**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**What is the expected behavior?**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**What is your host/environment?**\r\n - OS: [e.g. iOS]\r\n - Version [e.g. 22]\r\n - Plugins\r\n\r\n**Do you have any screenshots?**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context about the problem.\r\n",
            "comments": []
        },
        {
            "title": "[FEATURE] Support compound expression in Stats aggregation ",
            "description": "**Is your feature request related to a problem?**\r\nQuery  `stats max(age) by country` works but `stats max(age) / 2 by country` doesn't.\r\nQ8 in tpch:\r\n```sql\r\n\tsum(case\r\n\t\twhen nation = 'BRAZIL' then volume\r\n\t\telse 0\r\n\tend) / sum(volume) as mkt_share\r\n```\r\ncould be straightforwardly rewritten to\r\n```sql\r\nstats sum(case(nation = 'BRAZIL', volume else 0)) / sum(volume) as mkt_share by o_year\r\n```\r\nBut we haven't support compound expression in aggregation. So as a workaround, we have to change it to:\r\n```sql\r\n| stats sum(case(nation = 'BRAZIL', volume else 0)) as sum_case, sum(volume) as sum_volume by o_year\r\n| eval mkt_share = sum_case / sum_volume\r\n| fields mkt_share, o_year\r\n```\r\n\r\n**What solution would you like?**\r\nA clear and concise description of what you want to happen.\r\n\r\n**What alternatives have you considered?**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context or screenshots about the feature request here.",
            "comments": []
        },
        {
            "title": "[Fast-Follow-up] Add more ITs around index monitor",
            "description": "**Is your feature request related to a problem?**\r\nhttps://github.com/opensearch-project/opensearch-spark/pull/802#pullrequestreview-2393606495\r\n\r\n**What solution would you like?**\r\nAdd more ITs around index monitor\r\n\r\n**What alternatives have you considered?**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context or screenshots about the feature request here.",
            "comments": []
        },
        {
            "title": "[BUG] OpenSearch API - {\"message\":\"{\\\"Message\\\":\\\"Fail to run query. Cause: Not a map type: string\\\"}\"}",
            "description": "**What is the bug?**\r\nI tried to execute a PPL query using the API, and I got the following error: `{\"message\":\"{\\\"Message\\\":\\\"Fail to run query. Cause: Not a map type: string\\\"}\"}`\r\n\r\nThe same query would return results successfully in an OpenSearch domain.\r\n\r\n**How can one reproduce the bug?**\r\nSteps to reproduce the behavior:\r\n1. Issue an API request to include a query similar to this one:\r\n```\r\nsource = amazon_security_lake_glue_db_us_west_2.amazon_security_lake_table_us_west_2_lambda_execution_2_0 | where time_dt >= timestamp('\"'\"'2024-10-07'\"'\"') | fields accountid, region | head 10\r\n```\r\n**What is the expected behavior?**\r\nThe query should return 10 rows of data that include \"accountid\", and \"region\" as columns.\r\n\r\n**What is your host/environment?**\r\n - OS: [e.g. iOS]\r\n - Version [e.g. 22]\r\n - Plugins\r\n\r\n**Do you have any screenshots?**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context about the problem.\r\n",
            "comments": []
        },
        {
            "title": "[FEATURE]PPL aggregation Performance enhancement using sampling",
            "description": "**Is your feature request related to a problem?**\r\nCurrently PPL's Top / Rare perform a full scan & order by to output the most common or rare values:\r\n\r\n```sql\r\nSELECT \r\n    status_code AS Field, \r\n    COUNT(*) AS Frequency\r\nFROM \r\n    table\r\nGROUP BY \r\n    status_code\r\nORDER BY \r\n    Frequency DESC\r\nLIMIT 5\r\n```\r\nThis query represents the logical plan to be executed by the engine.\r\nIt has the inherited flaw of having to scan the entire table and order the results only to get the top 5 elements -  this is very costly .\r\n\r\nIn general any aggregation of select statement could benefit using a sampling strategy ...\r\n\r\n**What solution would you like?**\r\nIn many cases the overall cardinality of a column and its values can be determined using a small sample of the dataset.\r\nspark offers the next syntax for sampling the table:\r\n\r\n```sql\r\nTABLESAMPLE ({ integer_expression | decimal_expression } PERCENT)\r\n    | TABLESAMPLE ( integer_expression ROWS )\r\n    | TABLESAMPLE ( BUCKET integer_expression OUT OF integer_expression )\r\n```\r\n\r\nAnd in our case:\r\n\r\n```sql\r\nSELECT \r\n    status_code AS Field, \r\n    COUNT(*) AS Frequency\r\nFROM \r\n    t TABLESAMPLE (10 precent) -- Get approximately 10% of the rows\r\nGROUP BY \r\n    status_code\r\nORDER BY \r\n    Frequency DESC\r\nLIMIT 5;\r\n```\r\n\r\nThe new `top` and `rare` api will look as follows:\r\n\r\n`top [N] <field-list> [by-clause] [TABLESAMPLE ({ integer_expression | decimal_expression } PERCENT)]`\r\n\r\nExamples:\r\n\r\n```sql\r\nsource=accounts | top 5 age by gender tablesample (10 precent)\r\nsource=accounts | rare 5 age by nationality tablesample (500 rows)\r\nsource=accounts | rare 10 nationality tablesample(bucket 4 out of 10);\r\n```\r\n\r\n---\r\n**Do you have any additional context?**\r\n- https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sampling.html\r\n- https://github.com/opensearch-project/opensearch-spark/issues/740\r\n",
            "comments": []
        },
        {
            "title": "[BUG] Auto-refresh materialized view / covering index doesn't ingest all the data within the specified timeframe",
            "description": "**What is the bug?**\r\nWhen I was creating an auto-refresh materialized view, the statement I used included a where clause that looks for all the logs in the past '1' day. However, I noticed that each refresh didn't ingest logs all the way back within the specified timeframe, and only partial logs were ingested.\r\n\r\n**How can one reproduce the bug?**\r\nSteps to reproduce the behavior:\r\n1. Create a materialized view / covering index and set auto_refresh=true using a statement similar to this:\r\n\r\n```\r\nCREATE MATERIALIZED VIEW testing_auto_mv AS\r\n    SELECT time_dt,\r\n        actor,\r\n        accountid,\r\n        region,\r\n        src_endpoint,\r\n        api,\r\n        http_request,\r\n        is_mfa,\r\n        class_uid,\r\n        resources\r\n     FROM securitylake.amazon_security_lake_glue_db_us_east_1.amazon_security_lake_table_us_east_1_cloud_trail_mgmt_2_0\r\n     WHERE time_dt BETWEEN CURRENT_TIMESTAMP - INTERVAL '1' DAY AND CURRENT_TIMESTAMP\r\n        AND accountid in ('{account id}')\r\n        AND region = 'us-east-1' \r\nWITH ( auto_refresh = true, refresh_interval = '15 Minute', checkpoint_location = 's3://{bucket name}/AWSLogs/checkpoint_test')\r\n```\r\n2. Create a materialized view and set auto_refresh=false using the same statement:\r\n```\r\nCREATE MATERIALIZED VIEW testing_manual_mv AS\r\n    SELECT time_dt,\r\n        actor,\r\n        accountid,\r\n        region,\r\n        src_endpoint,\r\n        api,\r\n        http_request,\r\n        is_mfa,\r\n        class_uid,\r\n        resources\r\n     FROM securitylake.amazon_security_lake_glue_db_us_east_1.amazon_security_lake_table_us_east_1_cloud_trail_mgmt_2_0\r\n     WHERE time_dt BETWEEN CURRENT_TIMESTAMP - INTERVAL '1' DAY AND CURRENT_TIMESTAMP\r\n        AND accountid in ('{account id}')\r\n        AND region = 'us-east-1' \r\nWITH ( auto_refresh = false)\r\n```\r\nAnd run `REFRESH testing_manual_mv`\r\n\r\n3. Run the following SQL query against both MVs you just created, and compare the earliest timestamp:\r\n```\r\nSELECT time_dt FROM flint_index ORDER BY time_dt LIMIT 1\r\n```\r\nYou can also check the difference of number of documents that are ingested in both MVs.\r\n\r\n**What is the expected behavior?**\r\nThe auto-refresh MV should ingest all the logs within the specified timeframe\r\n\r\n**What is your host/environment?**\r\n - OS: [e.g. iOS]\r\n - Version 2.13\r\n - Plugins\r\n\r\n**Do you have any screenshots?**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context about the problem.\r\n",
            "comments": [
                {
                    "user": "dai-chen",
                    "comment": "Could you help quick check the oldest `time_dt` ingested in MV data? Just wonder if any time zone issue here."
                },
                {
                    "user": "A-Gray-Cat",
                    "comment": "@dai-chen Yes. The MV was created around 12:09PM PDT, and the oldest `time_dt` value in the MV is `2024-10-17 13:07:31` in UTC I would assume."
                }
            ]
        },
        {
            "title": "PPL geoip function",
            "description": "### Description\r\nPPL geoip function\r\n\r\n### Issues Resolved\r\nhttps://github.com/opensearch-project/opensearch-spark/issues/672\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "salyh",
                    "comment": "@YANG-DB @penghuo @lukasz-soszynski-eliatra @kt-eliatra @dr-lilienthal PPL geoip function syntax proposal\r\n\r\nPlease review and comment if needed"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@salyh thanks\r\non my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource,  can/should the `datasource` be one of the following:\r\n - external API\r\n - table\r\n - file \r\n Lets add these capabilities ..."
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin @penghuo @dai-chen please add u'r comments here"
                },
                {
                    "user": "salyh",
                    "comment": "> @salyh thanks on my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource, can/should the `datasource` be one of the following:\r\n> \r\n>     * external API\r\n> \r\n>     * table\r\n> \r\n>     * file\r\n>       Lets add these capabilities ...\r\n\r\n@YANG-DB @LantaoJin @penghuo @dai-chen \r\n\r\ndatasource refers to the datasources provided by the ip2geo processor https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/\r\n\r\nMy idea was to leveage the already present capabilities of the [ip2geo processor](https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/) and extend it as necessary. Because it would avoid code duplication. When I read the docs of the processor correct, GeoLite2 is supported by now. I propose to add the above requested functionality to the processor. Then we try to use the code from the processor (as a library) to solve this issue. Otherwise we would reinvent the wheel.\r\n\r\nAnd there is also https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/geoip/ in data prepper which could be facilitated"
                },
                {
                    "user": "YANG-DB",
                    "comment": "> > @salyh thanks on my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource, can/should the `datasource` be one of the following:\r\n> > ```\r\n> > * external API\r\n> > \r\n> > * table\r\n> > \r\n> > * file\r\n> >   Lets add these capabilities ...\r\n> > ```\r\n> \r\n> @YANG-DB @LantaoJin @penghuo @dai-chen\r\n> \r\n> datasource refers to the datasources provided by the ip2geo processor https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/\r\n> \r\n> My idea was to leveage the already present capabilities of the [ip2geo processor](https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/) and extend it as necessary. Because it would avoid code duplication. When I read the docs of the processor correct, GeoLite2 is supported by now. I propose to add the above requested functionality to the processor. Then we try to use the code from the processor (as a library) to solve this issue. Otherwise we would reinvent the wheel.\r\n> \r\n> And there is also https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/geoip/ in data prepper which could be facilitated\r\n\r\n@salyh this sounds like a good idea !\r\n@LantaoJin @penghuo - any comments ?"
                },
                {
                    "user": "kenrickyap",
                    "comment": "HI @salyh,\r\n\r\nCurrently looking into implementation if this feature. I saw above conversation on planning to leverage ip2geo functionality within geospatial plugin. Did you have a clear idea on how this can be done?\r\n\r\nFrom my understanding to leverage ip2geo, one must first create a ip2geo processor then attach it to a opensearch pipeline. I am not too sure how the Spark plugin could access this functionality. Some pointers would be much appreciated :)\r\n\r\nThanks!"
                },
                {
                    "user": "salyh",
                    "comment": "> HI @salyh,\r\n> \r\n> Currently looking into implementation if this feature. I saw above conversation on planning to leverage ip2geo functionality within geospatial plugin. Did you have a clear idea on how this can be done?\r\n> \r\n> From my understanding to leverage ip2geo, one must first create a ip2geo processor then attach it to a opensearch pipeline. I am not too sure how the Spark plugin could access this functionality. Some pointers would be much appreciated :)\r\n> \r\n> Thanks!\r\n\r\nThe idea is not to leverage the processor as a processor but to reuse its code (either by copying it - not ideal - , or by separating them into a kind of shared commons library or by adding them as a dependency on code level)."
                },
                {
                    "user": "kenrickyap",
                    "comment": "Sounds good my current idea is to create an spi for the plugin, as was done for job-scheduler (allowing the geospatial plugin to implement ExtensiblePlugin) am still working on a design. This is to allow the sql plugin to access the ip2geo functionality (am also working on the iplocation functionality there).\r\n\r\nWould you know if the Spark project would be able to leverage the SPI since it is not a plugin? If we are to expose the ip2geo functionality I would rather create something that works both for the sql-plugin and Spark."
                },
                {
                    "user": "kenrickyap",
                    "comment": "Hi @salyh, spent a bit more time investigating this ticket. currently plan is to rewrite the geo2ip functionality within the spark project. The reason for this is that currently the geo2ip processor with the geospatial plugin is intwined with using the job scheduler extension. From my understanding there is no way to leverage usage of this extension in spark so a lot of the functionality would not be able to be migrated.\r\n\r\nThe rewritten geo2ip within spark will:\r\n- create spark temp table storing data from geoip datasource if table does not already exist\r\n- retrieve information from said table based on provided ips.\r\n\r\nThe current design question is if it would be better to use spark UDF or reflection to call [java_method](https://spark.apache.org/docs/3.5.3/api/sql/index.html#java_method) to call the rewritten geo2ip functionality?"
                },
                {
                    "user": "kenrickyap",
                    "comment": "> Hi @salyh, spent a bit more time investigating this ticket. currently plan is to rewrite the geo2ip functionality within the spark project. The reason for this is that currently the geo2ip processor with the geospatial plugin is intwined with using the job scheduler extension. From my understanding there is no way to leverage usage of this extension in spark so a lot of the functionality would not be able to be migrated.\r\n> \r\n> The rewritten geo2ip within spark will:\r\n> \r\n> * create spark temp table storing data from geoip datasource if table does not already exist\r\n> * retrieve information from said table based on provided ips.\r\n> \r\n> The current design question is if it would be better to use spark UDF or reflection to call [java_method](https://spark.apache.org/docs/3.5.3/api/sql/index.html#java_method) to call the rewritten geo2ip functionality?\r\n\r\nWill follow CIDR https://github.com/opensearch-project/opensearch-spark/pull/706/files approach and use UDF"
                }
            ]
        },
        {
            "title": "[FEATURE] Support index mapping option in create index statement",
            "description": "**Is your feature request related to a problem?**\r\n\r\nCurrently, users cannot customize the index mapping for the OpenSearch index that stores the Flint index data created via Spark SQL.\r\n\r\n**What solution would you like?**\r\n\r\nSimilarly as `index_settings` option introduced previously, a new option `index_mappings` can be added to the create statement in Spark SQL. This would allow users fine grain control over the OpenSearch index mappings.\r\n\r\nFor example:\r\n\r\n```\r\nCREATE MATERIALIZED VIEW mv_test\r\nAS\r\nSELECT ... \r\nWITH (\r\n  index_mappings: '{ \"_source\": { \"enabled\": false } }'\r\n)\r\n```\r\n\r\nNote that for the implementation, the index schema generated by the Flint index must be merged with the schema provided in the `index_mappings` option.\r\n\r\n**What alternatives have you considered?**\r\n\r\nManually modifying the OpenSearch index after the materialized view is created. However, this approach won\u2019t work for configurations that need to be set before indexing begins.\r\n\r\n**Do you have any additional context?**\r\n\r\nThis feature would provide users the flexibility to optimize the OpenSearch index according to their specific needs. For example, it would allow them to:\r\n\r\n1. Disable the _source field to save space.\r\n2. Disable doc_values for specific fields to save space.\r\n3. Force certain field types (e.g., IP type) as a temporary workaround until SparkSQL supports them.",
            "comments": []
        },
        {
            "title": "[FEATURE] Cost-effective materialized view for high cardinality data",
            "description": "## Is your feature request related to a problem?\r\n\r\nWhen dealing with high cardinality data, Materialized Views (MVs) can become excessively large and inefficient, leading to significant performance and cost challenges. Take VPC Flow dashboard as an example, high cardinality fields like source and destination IP pairs create significant storage challenges when using MVs. At the terabyte (TB) scale, each 1-minute window can result in hundreds of millions of rows after grouping.\r\n\r\n<img width=\"1423\" alt=\"Screenshot 2024-11-05 at 9 23 44\u202fAM\" src=\"https://github.com/user-attachments/assets/eb10741a-2415-4b84-b8c6-4051d050b79f\">\r\n&nbsp;\r\n\r\nMaterialized view definition for VPC flow dashboard:\r\n\r\n<pre>\r\nCREATE MATERIALIZED VIEW vpc_flow_log_mv\r\nAS\r\nSELECT\r\n  window.start AS startTime,\r\n  activity_name,\r\n  protocol,\r\n  src_endpoint.ip AS src_ip,\r\n  dst_endpoint.ip AS dst_ip,\r\n  dst_endpoint.port AS dst_port,\r\n  COUNT(*) AS total_count,\r\n  SUM(traffic.bytes) AS total_bytes,\r\n  SUM(traffic.packets) AS total_packets\r\nFROM vpc_flow_logs\r\nGROUP BY\r\n  TUMBLE(eventTime, '1 Minute'),\r\n  <b>activity_name,\r\n  protocol,\r\n  src_endpoint.ip,\r\n  dst_endpoint.ip,\r\n  dst_endpoint.port</b>\r\n</pre>\r\n\r\n### Cost Breakdown\r\n\r\nThe cost of maintaining MVs for high-cardinality data primarily arises from three areas:\r\n\r\n1. **Creation Cost**\r\n    - a) Computation: High-cardinality fields demand intensive computation for data aggregation, driving up processing cost.\r\n    - b) Indexing: Writing to complex storage formats, such as Lucene indexing, adds further cost due to the resources required.\r\n    - Key factors: **Complexity of MV definition and choice of target file format.**\r\n2. **Storage Cost**\r\n    - Large data volumes in high-cardinality views increase storage expenses and lead to inefficiencies in data retrieval and maintenance.\r\n    - Key factors: **Volume of MV data and choice of storage medium.**\r\n3. **Query Cost**\r\n    - Querying high-cardinality views can be costly, as the size and complexity of the data require more resources to maintain responsiveness for real-time analytics.\r\n    - Key factors: **Frequency of dashboard queries on MV data and its computational complexity.**\r\n\r\n## What solution would you like?\r\n\r\n### Solution Overview\r\n\r\nTo address the challenges associated with high-cardinality data and dashboard visualizations, we are considering three primary solutions: Aggregate MV, Approximate MV, and Direct Querying. The table below provides a baseline comparison of the cost implications for each solution without additional optimizations.\r\n\r\n![Screenshot 2024-11-06 at 2 43 56\u202fPM](https://github.com/user-attachments/assets/0abcb23d-8c1a-4441-a9ef-3067bbcff67a)\r\n\r\n### Design Goals\r\n\r\nOur primary objective is to reduce **the overall cost of maintaining MVs** by optimizing these three aspects \u2014 creation, storage, and query \u2014 specifically for high-cardinality fields and dashboard visualizations.\r\n\r\nBeyond cost reduction, we aim for a balanced solution that effectively manages **the trade-offs between cost, latency, and accuracy**. This involves addressing the specific challenges of each approach, such as reducing storage requirements in Aggregate MV, enhancing accuracy in Approximate MV, and improving query latency in Direct Querying. \r\n\r\n<img width=\"481\" alt=\"Screenshot 2024-11-04 at 9 49 55\u202fAM\" src=\"https://github.com/user-attachments/assets/9b149188-906a-4be3-994a-7321f6ff152d\">\r\n\r\n### Proposed Path Forward [TBD]\r\n\r\nThis recommended path forward includes both short-term enhancements to the existing aggregate MV and longer-term solutions that better align with product goals. For more details on each approach, refer to the following **Additional Context** section.\r\n\r\n- **Short-Term (Aggregate MV)**\r\n    - Implement partial materialization with optional on-demand backfilling.\r\n    - Optimize OpenSearch index to reduce indexing resource usage.\r\n\r\n- **Long-Term**\r\n    - **MV on Object Store**: Explore the possibility of using a cost-effective object store to hold an MV.\r\n\r\n- **Long-Term (Product Alignment Required)**\r\n    - **Approximate MV**: Transition to an approximate approach, which may involve adjustments to the product definition.\r\n    - **Direct Querying**: Evaluate direct querying from the dashboard on the source dataset, requiring product validation to confirm the experience aligned with user expectations.\r\n\r\n---\r\n*Note: The following section provides detailed technical context. Continue if you\u2019re interested in specifics ...*\r\n\r\n## Do you have any additional context?\r\n\r\n### (I) Aggregate Materialized View\r\n\r\nThe most straightforward approach follows the core concept of an aggregate materialized view, with further optimizations reducing its size and cost while maintaining accuracy.\r\n\r\n- **a) Partial Materialization**: Generate the MV only for time ranges selected by users. For data outside these ranges, utilize direct querying or backfilling from the source to minimize storage needs.\r\n- **b) Optimize OpenSearch Storage**: Apply techniques such as selective field indexing and compression to reduce the storage footprint within OpenSearch.\r\n- **c) Lower Cost Storage**: Store MV data in cost-effective storage solutions, like object stores (e.g. S3). Use either OpenSearch-compatible formats (e.g., searchable snapshots) or alternative formats accessible via Spark.\r\n- d) Hybrid Aggregation with Direct Querying: Materialize only low-cardinality columns in the MV, while using direct queries for high-cardinality data.\r\n\r\n#### a) Partial Materialized View with Backfilling\r\n\r\n![Screenshot 2024-11-06 at 3 06 20\u202fPM](https://github.com/user-attachments/assets/d822e6ba-27fd-41e8-970a-704a62f0a50e)\r\n\r\n- **User Workflow**\r\n    - Users select an initial time range for real-time analytics, prompting an auto-refresh MV to refresh data from the source.\r\n    - When users extend the time range to an earlier start date, it triggers backfill loading, pulling older data into the MV.\r\n\r\n- **Cons**\r\n    - **Limited Benefits for Long-Term Analysis**: This approach is beneficial for users focused on analyzing recent data. However, it may be less effective for those requiring analysis across the entire dataset or over longer time ranges, such as a full year.\r\n    - **Potential Dashboard Delays**: Users may experience delays when using the dashboard, even after the initial bootstrap, as backfill operations may happen during their analytics. This could impact the real-time responsiveness of the dashboard.\r\n\r\n- **Technical Challenges**\r\n    - **Backfilling Job Management**\r\n        - Abstract the backfill loading process as part of the auto-refresh MV, reflecting it in index state and metadata. For instance, update the MV start time upon backfill completion, which is required for future query optimization.\r\n        - Avoid or manage multiple backfill jobs triggered in parallel to prevent resource contention.\r\n    - **Overlap Time Window Handling**\r\n        - Handle overlaps between time windows loaded by both auto-refresh and backfill processes to ensure data consistency and accuracy.\r\n    - Similar challenges discussed in\r\n        - https://github.com/opensearch-project/opensearch-spark/issues/29\r\n        - https://github.com/opensearch-project/opensearch-spark/issues/90\r\n\r\n#### b) OpenSearch Index Optimizations\r\n\r\nRegardless of the chosen approach, additional optimizations can be applied within the OpenSearch index to further reduce both indexing computation and storage requirements. **It's essential to validate these optimizations through real testing to ensure they translate into measurable cost reductions.** \r\n\r\n- **Disable `_source`**: Disabling the `_source` field can reduce the index size by approximately 40%, though it comes with side effects such as the loss of certain search functionalities like highlighting. This approach sacrifices some flexibility for better storage efficiency.\r\n\r\n- **Disable `docvalues` for non-aggregated fields**: For fields used only for filtering on the dashboard, `docvalues` can be disabled to save storage, as they are not required for aggregation.\r\n\r\n- **Disable inverted index on non-dimension fields**: If the index is primarily used to serve pre-canned dashboards, you can consider disabling the inverted index on non-dimension fields to save storage. This is particularly useful if these fields do not need to be used in filters, though they can still be used in aggregations.\r\n\r\n- **Store by the most appropriate field type**: Storing fields using the most fitting data types can significantly reduce the storage size. For example, instead of storing as a `keyword` field, using the `ip` type for IPv4 addresses which are very common in VPC flow log, CloudTrail, WAF dataset.\r\n\r\n- **Change compression rate**: Adjusting the compression settings in OpenSearch from the default to the best compression option can further reduce the index size, at the cost of slightly slower indexing speeds.\r\n\r\nThe last item can be configured in [`index_settings` option](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/index.md#create-index-options) while the first two will be configurable once support for https://github.com/opensearch-project/opensearch-spark/issues/772 is implemented.\r\n\r\n#### c) Materialized View on Cost-Effective Object Store\r\n\r\n![Screenshot 2024-11-06 at 3 06 31\u202fPM](https://github.com/user-attachments/assets/3250ad7f-0430-45b0-86ce-58e48b6cd05f)\r\n\r\n- **Cons** \r\n   - **Warmup Latency**: Initial queries on uncached segments may experience higher latency, as data segments are loaded on demand from the object store.\r\n  - **Read-Only Limitations**: Searchable snapshots create a read-only index, limiting flexibility for any updates or modifications to the MV.\r\n\r\n- **Technical Challenges**\r\n    - **Index Cache Tuning**: Efficiently managing the LRU cache is essential to balance memory usage and query performance, including tuning cache algorithm, size and eviction policies.\r\n  - **Snapshot Format Compatibility**: Writing data from Spark jobs compatible with OpenSearch\u2019s snapshot format presents an open question.\r\n\r\n### (II) Approximate Materialized View\r\n\r\nThe basic idea is to create MVs that store approximate or partial data, significantly reducing storage requirements by capturing only the most essential information while accepting trade-offs in accuracy or completeness.\r\n\r\n- **a) Approximate Aggregation**: Employ approximate data structures to handle high cardinality data. Specifically, these stream-friendly SQL functions process data without requiring sorting and compress the data by storing approximate values, such as Top K by frequency, or Top K by sum.\r\n- **b) Iceberg-Cube Aggregation**: This approach groups the source data by different dimension combinations (e.g., source IP, destination IP, source-destination IP pair) and selectively materializes only a subset of the cells in these large groups. By applying threshold such as Top K most frequent, it can decide which cells to materialize, significantly reducing storage while still capturing the most relevant data for querying.\r\n- c) Cluster-based Aggregation: This approach reduces the cardinality of high-cardinality columns by grouping similar values into clusters, such as clustering specific IPs into IP ranges or geographical locations (e.g., countries or regions). By aggregating data into these broader clusters, the materialized view significantly reduces the number of rows, optimizing storage while still providing meaningful data for analysis.\r\n\r\n<img width=\"1412\" alt=\"Screenshot 2024-11-05 at 10 43 37\u202fAM\" src=\"https://github.com/user-attachments/assets/7b7e4f82-d804-4570-9c87-157f92cdf7c7\">\r\n\r\n- **Cons**\r\n    - **Reduced Accuracy over Aggregated Windows**: Approximate or partial results are stored for fine-grained windows, such as 1-minute intervals. However, dashboards often need to aggregate this data into larger windows, such as 5-minute intervals. Aggregating such results over these larger windows can introduce errors or reduce the precision of the data, making it harder to trust the insights.\r\n    - **Overhead on small datasets**: High cardinality columns are duplicate in each approximate aggregate result or cube cells (Top K src_ip by count, Top K src_ip by bytes). This duplication increases the overhead, especially for small datasets, as the approximation structures must now store multiple instances of these columns, potentially inflating the data size and reducing the storage efficiency benefits.\r\n\r\n- **Technical Challenges**\r\n    - **Complex Computation Limitations in Spark Streaming**: The cubing approach in 2b requires advanced operations, such as window functions to calculate top-k per group. However, Spark Structured Streaming currently lacks support for these complex computations, and how to make it work remains an open question.\r\n    - **Lack of Built-in Approximate Top-K Function**: Spark does not have a built-in approximate top-k function, so this functionality would need to be implemented from scratch, adding complexity and development effort.\r\n    - **Frontend Adaptation for New MV Structure**: The new MV structure, which may include top-k results in nested fields or sparse indexing from data cubing, will require corresponding changes in the frontend code to handle these structures properly.\r\n\r\n### (III) Direct Querying Dashboard\r\n\r\nPerform direct queries on the source data for real-time analysis, similar to Grafana and CloudWatch Log Insights experience, without relying on pre-computed MV.\r\n\r\n![Screenshot 2024-11-06 at 3 06 44\u202fPM](https://github.com/user-attachments/assets/c0078124-b699-4d49-9475-8c48588473f0)\r\n\r\n- **Pros**\r\n  - **Realtime Data Access**: Users can query the most up-to-date data directly from the source.\r\n  - **Reduced Storage Costs**: Eliminates the need for persistent storage of MVs.\r\n  - **Flexibility**: Allows for a wide range of queries without being limited by the predefined structure of MVs.\r\n\r\n- **Cons**\r\n  - **Query Latency**: Direct querying can result in slower response times, especially with large datasets or complex queries, impacting the responsiveness of the dashboard.\r\n  - **Resource Intensity**: Frequent querying can place a higher load on the source data query engine and storage.\r\n\r\n- **Technical Challenges**\r\n    - **Automatic Acceleration**: Leverage Query Result Cache for repeated, deterministic queries (e.g., same query with varying time ranges) or MV created dynamically based on workload.\r\n    - **Progressive Execution**: Deliver incremental, partial results to the dashboard, improving responsiveness by displaying data as it\u2019s processed.\r\n    - **Approximate Functions**: Apply approximate calculations (e.g., approximate counts or top-k) to speed up response times, similar to the techniques used in approximate MVs.\r\n",
            "comments": [
                {
                    "user": "dai-chen",
                    "comment": "## Proof of Concept: Approximate Aggregation Approach\r\n\r\n### Goals\r\n\r\nVerify the feasibility of the Approximate Aggregation MV approach and evaluate its impact on storage, performance and cost, specifically including:\r\n\r\n- **Implement approximate aggregate functions** such as `approx_top_count` and `approx_top_sum`.\r\n- **Compare actual storage in the OpenSearch index** between traditional aggregate MV and approximate aggregate MV.\r\n- **Benchmark MV build and dashboard query performance** between traditional aggregate MV and approximate aggregate MV.\r\n- **Evaluate EMR-S job costs** for building the materialized view using traditional aggregation versus approximate aggregation.\r\n\r\n### Design\r\n\r\nSyntax:\r\n\r\n<pre>\r\nCREATE MATERIALIZED VIEW vpc_flow_log_mv\r\nAS\r\nSELECT\r\n  window.start AS startTime,\r\n  activity,\r\n  <b>APPROX_TOP_COUNT(src_endpoint.ip, 100) AS top_k_src_ip_by_count,\r\n  APPROX_TOP_COUNT(dst_endpoint.ip, 100) AS top_k_dst_ip_by_count,\r\n  APPROX_TOP_SUM(src_endpoint.ip, 100) AS top_k_src_ip_by_sum,\r\n  APPROX_TOP_SUM(dst_endpoint.ip, 100) AS top_k_dst_ip_by_sum,\r\n  APPROX_TOP_COUNT(ARRAY(src_endpoint.ip, dst_endpoint.ip), 100) AS top_k_src_dst_ip_by_count,</b>\r\n  COUNT(*) AS total_count,\r\n  SUM(traffic.bytes) AS total_bytes,\r\n  SUM(traffic.packets) AS total_packets\r\nFROM vpc_flow_logs\r\n<b>GROUP BY\r\n  TUMBLE(eventTime, '1 Day'),\r\n  activity_name</b>\r\n</pre>\r\n\r\nMaterialized view data:\r\n\r\n<pre>\r\n    \"_source\": {\r\n          \"startTime\": \"2024-10-01 12:00:00\",\r\n          \"activity\": \"Traffic\",\r\n          <b>\"top_k_src_ip_by_count\": [\r\n            {\r\n              \"ip\": \"192.168.0.100\",\r\n              \"count\": 23205\r\n            },\r\n            ...\r\n           ]\r\n           \"top_k_dst_ip_by_count\": [\r\n            {\r\n              \"ip\": \"127.0.01\",\r\n              \"count\": 238\r\n            }</b>\r\n            ...\r\n           ]\r\n        },\r\n</pre>\r\n\r\nOpenSearch DSL query:\r\n\r\n<pre>\r\nPOST /vpc_flow_log_approx_mv/_search\r\n{\r\n  \"size\": 0,\r\n  \"aggs\": {\r\n    \"top_ips\": {\r\n      \"nested\": {\r\n        \"path\": \"top_k_src_ip_by_count\"\r\n      },\r\n      \"aggs\": {\r\n        \"ip_buckets\": {\r\n          <b>\"terms\": {\r\n            \"field\": \"top_k_src_ip_by_count.ip\",</b>\r\n            \"size\": 100,\r\n            \"order\": {\r\n              \"total_count\": \"desc\"\r\n            }\r\n          },\r\n          \"aggs\": {\r\n            \"total_count\": {\r\n              <b>\"sum\": {\r\n                \"field\": \"top_k_src_ip_by_count.count\"</b>\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n</pre>\r\n\r\n### Implementation Tasks\r\n\r\n1. **Implement `approx_top_count` function**: Create a function to compute approximate top K counts for high cardinality fields.\r\n2. **Implement `approx_top_sum` function**: Develop a similar function for approximate top K sum calculations.\r\n3. **Support nested fields in MV output**: Ensure the materialized view (MV) can output nested fields to store approximate aggregation results.\r\n4. **Create a dashboard on MV data**: Build a dashboard for visualizing the results from the MV, using approximate aggregation for top K values.\r\n\r\n### Testing Tasks\r\n\r\n1. **Storage comparison**: Compare the actual storage usage in the OpenSearch index for aggregate MV and approximate aggregate MV.\r\n2. **Performance benchmark**: Measure and compare the performance of building and querying aggregate MV and approximate aggregate MV.\r\n3. **EMR-S cost evaluation**: Evaluate the overall cost on EMR-S for building aggregate MV versus approximate aggregate MV.\r\n"
                },
                {
                    "user": "dai-chen",
                    "comment": "## Proof of Concept: OpenSearch Index Optimizations\r\n\r\nTODO: A quick test showed a 58% reduction in index size by disabling _source and unnecessary inverted indexes.\r\n\r\n<pre>\r\nhealth status index              uuid                 pri rep docs.count docs.deleted store.size pri.store.size\r\n              vpc_flow_logs_mv_3 oG5_m5IB0HCHA9l1-54T            1000000            0     82.3mb         82.3mb\r\n              vpc_flow_logs_mv_5 o26Em5IB0HCHA9l1F56g            1000000            0     59.2mb         59.2mb\r\n              vpc_flow_logs_mv_1 _xJzm5IB1Vv8TQuVfOXQ            1000000            0    140.9mb        140.9mb\r\n\r\n# Current schema (baseline)\r\nPUT vpc_flow_logs_mv_1\r\n{\r\n  \"settings\": {\r\n    \"number_of_shards\": 1,\r\n    \"number_of_replicas\": 2\r\n  }, \r\n  \"mappings\": {\r\n    \"properties\": {\r\n      \"startTime\": { \"type\": \"date\" },\r\n      \"activity\": { \"type\": \"keyword\" },\r\n      \"src_ip\": { \"type\": \"keyword\" },\r\n      \"dest_ip\": { \"type\": \"keyword\" },\r\n      \"total_count\": { \"type\": \"long\" },\r\n      \"total_bytes\": { \"type\": \"long\" },\r\n      \"total_packets\": { \"type\": \"long\" }\r\n    }\r\n  }\r\n}\r\n\r\n# Disable source\r\nPUT vpc_flow_logs_mv_3\r\n{\r\n  \"settings\": {\r\n    \"number_of_shards\": 1,\r\n    \"number_of_replicas\": 2\r\n  }, \r\n  \"mappings\": {\r\n    \"_source\": {\r\n      \"enabled\": false\r\n    },\r\n    \"properties\": {\r\n      \"startTime\": { \"type\": \"date\" },\r\n      \"activity\": { \"type\": \"keyword\" },\r\n      \"src_ip\": { \"type\": \"keyword\" },\r\n      \"dest_ip\": { \"type\": \"keyword\" },\r\n      \"total_count\": { \"type\": \"long\" },\r\n      \"total_bytes\": { \"type\": \"long\" },\r\n      \"total_packets\": { \"type\": \"long\" }\r\n    }\r\n  }\r\n}\r\n\r\n# Combine all\r\nPUT vpc_flow_logs_mv_5\r\n{\r\n  \"settings\": {\r\n    \"number_of_shards\": 1,\r\n    \"number_of_replicas\": 2\r\n  }, \r\n  \"mappings\": {\r\n    \"_source\": {\r\n      \"enabled\": false\r\n    }, \r\n    \"properties\": {\r\n      \"startTime\": { \"type\": \"date\" },\r\n      \"activity\": { \"type\": \"keyword\" },\r\n      \"src_ip\": { \"type\": \"ip\" },\r\n      \"dest_ip\": { \"type\": \"ip\" },\r\n      \"total_count\": { \"type\": \"long\", \"index\": false },\r\n      \"total_bytes\": { \"type\": \"long\", \"index\": false },\r\n      \"total_packets\": { \"type\": \"long\", \"index\": false }\r\n    }\r\n  }\r\n}\r\n</pre>"
                }
            ]
        },
        {
            "title": "[BUG] timestamp comparison using \"INTERVAL\" literal doesn't work against ingested data",
            "description": "**What is the bug?**\r\nWhen running a direct query, the following query would work and return results\r\n```\r\nSELECT *\r\nFROM table1\r\nWHERE time_dt BETWEEN CURRENT_TIMESTAMP - INTERVAL '7' DAY AND CURRENT_TIMESTAMP\r\nLIMIT 10\r\n```\r\nHowever, when running the same query against a materialized view, covering index, or ingested data, it would error out. After removing the timestamp where clause, the query would go through:\r\n`Bad Request, this query is not runnable.`\r\n\r\n**How can one reproduce the bug?**\r\nSteps to reproduce the behavior:\r\n1. Add `WHERE timestamp BETWEEN CURRENT_TIMESTAMP - INTERVAL '7' DAY AND CURRENT_TIMESTAMP` to any query, and run it against a flint table.\r\n2. Create a materialized view from the same flint table.\r\n3. Run the same query against the materialized view you just created, and the timestamp filter will make the query error out.\r\n\r\n**What is the expected behavior?**\r\nThe timestamp comparison should work for both flint and local index.\r\n\r\n**What is your host/environment?**\r\n - OS: [e.g. iOS]\r\n - Version 2.13, and I also ran the query in future.playground against ingested data.\r\n - Plugins\r\n\r\n**Do you have any screenshots?**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Do you have any additional context?**\r\nAdd any other context about the problem.\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@penghuo @dai-chen could u plz take a look ?"
                }
            ]
        }
    ],
    "pull_requests": [
        {
            "title": "Support parenthesized expression in filter",
            "description": "### Description\r\nParenthesized expression cannot work as expected in filter:\r\n`source = $testTable | where (country = 'Canada' or age > 60) and age < 25 | fields name, age, country` failed in parser.\r\n\r\n### Related Issues\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/887\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [ ] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "LantaoJin"
        },
        {
            "title": "Apply shaded rules",
            "description": "### Description\r\n\r\nApply shaded rules to prevent potential jar hell issue \r\n\r\n### Check List\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "noCharger"
        },
        {
            "title": "Ppl count approximate support",
            "description": "### Description\r\nsupport approximation operations for \r\n - count distinct\r\n - top\r\n - rare\r\n \r\n### Related Issues\r\n#882 \r\n\r\n### related context\r\n- https://spark.apache.org/docs/3.5.2/sql-ref-functions-builtin.html\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [X] Implemented tests for combination with other commands\r\n- [X] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "YANG-DB"
        },
        {
            "title": "Fix bug for not able to get sourceTables from metadata",
            "description": "### Description\r\nUsing custom implementation for `FlintIndexMetadataService` exposes a bug where we incorrectly get empty `sourceTables` from `FlintMetadata`.\r\nDepending on how the `FlintMetadata` is generated, the `metadata.properties.get(\"sourceTables\")` could either be of type java array or scala array.\r\nThis PR fixes the bug, now considering both cases.\r\n\r\nAlso added some logs for troubleshooting in production environment.\r\n\r\n\r\n### Related Issues\r\n* #746 \r\n* #854 \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "seankao-az"
        },
        {
            "title": "Add `sample` parameter to `top` & `rare` command",
            "description": "### Description\r\nAdd a new sample command (`sample`) to reduce amount of scanned data points and allow approximation of a `top` or `rare` statements when faster sample based results if favour of exact long running results \r\n\r\n```sql\r\nsource = testTable  | rare address sample(50 percent)\r\nsource = testTable  | top 5 address by country sample(25 percent)\r\n```\r\n\r\n### Issues Resolved\r\n- https://github.com/opensearch-project/opensearch-spark/issues/740\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "LantaoJin",
                    "comment": "One high level question:\r\nHow do we determine the relationship between percentage and precision? Or how much precision does it lose when sampling is decreased from 100% to 80% or from 80% to 50%?\r\n\r\nI'm wondering what kind of scenario needs to run `top` on the sample data."
                }
            ],
            "owner": "YANG-DB"
        },
        {
            "title": "Add sanity test script",
            "description": "### Description\r\nThis Python script executes test queries from a CSV file using an asynchronous query API and generates comprehensive test reports.\r\n\r\nThe script produces two report types:\r\n1. An Excel report with detailed test information for each query\r\n2. A JSON report containing both test result overview and query-specific details\r\n\r\nApart from the basic feature, it also has some advanced functionality includes:\r\n1. Concurrent query execution (note: the async query service has session limits, so use thread workers moderately despite it already supports session ID reuse)\r\n2. Configurable query timeout with periodic status checks and automatic cancellation if timeout occurs.\r\n3. Flexible row selection from the input CSV file, by specifying start row and end row of the input CSV file.\r\n4. Expected status validation when expected_status is present in the CSV\r\n5. Ability to generate partial reports if testing is interrupted\r\nAn example to run the test script:\r\n```\r\npython SanityTest.py --base-url ${URL_ADDRESS} --username *** --password *** --datasource ${DATASOURCE_NAME} --input-csv test_queries.csv --output-file test_report --max-workers 2 --check-interval 10 --timeout 600\r\n```\r\nFor more details, you can see the help manual via command:\r\n```\r\npython SanityTest.py --help   \r\n\r\nusage: SanityTest.py [-h] --base-url BASE_URL --username USERNAME --password PASSWORD --datasource DATASOURCE --input-csv INPUT_CSV\r\n                                      --output-file OUTPUT_FILE [--max-workers MAX_WORKERS] [--check-interval CHECK_INTERVAL] [--timeout TIMEOUT]\r\n                                      [--start-row START_ROW] [--end-row END_ROW]\r\n\r\nRun tests from a CSV file and generate a report.\r\n\r\noptions:\r\n  -h, --help            show this help message and exit\r\n  --base-url BASE_URL   Base URL of the service\r\n  --username USERNAME   Username for authentication\r\n  --password PASSWORD   Password for authentication\r\n  --datasource DATASOURCE\r\n                        Datasource name\r\n  --input-csv INPUT_CSV\r\n                        Path to the CSV file containing test queries\r\n  --output-file OUTPUT_FILE\r\n                        Path to the output report file\r\n  --max-workers MAX_WORKERS\r\n                        optional, Maximum number of worker threads (default: 2)\r\n  --check-interval CHECK_INTERVAL\r\n                        optional, Check interval in seconds (default: 10)\r\n  --timeout TIMEOUT     optional, Timeout in seconds (default: 600)\r\n  --start-row START_ROW\r\n                        optionl, The start row of the query to run, start from 0\r\n  --end-row END_ROW     optional, The end row of the query to run\r\n```\r\n\r\n### Related Issues\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/877\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [ ] Implemented unit tests\r\n- [ ] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@qianheng-aws this looks good - please add\r\n- documentation with all the above description and instructions + results report example\r\n- what are the prerequisites for running the script - docker ? \r\n- what are the data prerequisites for running the script - import content process ?\r\n- export the results to a file including :\r\n   -  ppl version\r\n   -  performance time \r\n   -  coverage\r\n - plz also add a folder for CSV tests   "
                },
                {
                    "user": "qianheng-aws",
                    "comment": "> @qianheng-aws this looks good - please add\r\n> \r\n> * documentation with all the above description and instructions + results report example\r\n> * what are the prerequisites for running the script - docker ?\r\n> * what are the data prerequisites for running the script - import content process ?\r\n> * export the results to a file including :\r\n>   \r\n>   * ppl version\r\n>   * performance time\r\n>   * coverage\r\n> * plz also add a folder for CSV tests\r\n\r\nI addressed most part  of the comment and add them in the README.md, except some points:\r\n- the data prerequisites. I haven't got the background of where the data comes from and how to ingest them to flint yet. Left a TODO in the README.\r\n- ppl version. Do you mean the version returns from API `GET _cat/plugins?v`?  I think it should always be the same as the opensearch version. Will it help to add that in the report?\r\n"
                }
            ],
            "owner": "qianheng-aws"
        },
        {
            "title": "New trendline ppl command (WMA)",
            "description": "### Description\r\nIntroduce a new variant (WMA) for existing trendline ppl command, by compositing a logical plan similar to the following with function `nth_value( )` to calculate the WMA value by perform event look behind. \r\n\r\n```\r\n-- +- 'Project ['name, 'salary, \r\n-- (((('nth_value('salary, 3) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 3) + \r\n-- ('nth_value('salary, 2) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 2)) + \r\n-- ('nth_value('salary, 1) windowspecdefinition('age ASC NULLS FIRST, specifiedwindowframe(RowFrame, -2, currentrow$())) * 1)) / 6) AS WMA#708]\r\n   -- +- 'UnresolvedRelation [employees], [], false\r\n```\r\n\r\n\r\nSome high level code changes:\r\n - Update developer README to include selected set of Unit || Integration test\r\n - Update example for WMA command usage\r\n - Update CatelystQueryPlanVisotor related classes to provide sort option argument into `TrendLine` processing logic, as sort field is mandatory for WMA calculation.\r\n - Update `TrendlineCatalystUtils.java` to have a new code path for WMA selection and associated calculation logic.\r\n\r\n\r\n### Related Issues\r\nPrior implement for SMA formula: https://github.com/opensearch-project/opensearch-spark/pull/833\r\n\r\n\r\n### Check List\r\n- [x] Updated documentation (docs/ppl-lang/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] New added source code should include a copyright header\r\n- [x] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\r\n### Test plan:\r\nDespite the existing unit test / integration test, the feature can also be tested manually, by first inserting a simple table, then run PPL trend line command against the table to calculate WMA value.\r\n```\r\n# Produce the artifact\r\nsbt clean sparkPPLCosmetic/publishM2\r\n\r\n# Start Spark with the plugin\r\nbin/spark-sql --jars \"/ABSOLUTE_PATH_TO_ARTIFACT/opensearch-spark-ppl_2.12-0.6.0-SNAPSHOT.jar\" \\\r\n--conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintPPLSparkExtensions\"  \\\r\n--conf \"spark.sql.catalog.dev=org.apache.spark.opensearch.catalog.OpenSearchCatalog\" \\\r\n--conf \"spark.hadoop.hive.cli.print.header=true\"\r\n\r\n# Insert test table and data\r\nCREATE TABLE employees (name STRING, dept STRING, salary INT, age INT, con STRING);\r\n\r\nINSERT INTO employees VALUES (\"Lisa\", \"Sales------\", 10000, 35, 'test');\r\nINSERT INTO employees VALUES (\"Evan\", \"Sales------\", 32000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Fred\", \"Engineering\", 21000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Alex\", \"Sales\", 30000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Tom\", \"Engineering\", 23000, 33, 'test');\r\nINSERT INTO employees VALUES (\"Jane\", \"Marketing\", 29000, 28, 'test');\r\nINSERT INTO employees VALUES (\"Jeff\", \"Marketing\", 35000, 38, 'test');\r\nINSERT INTO employees VALUES (\"Paul\", \"Engineering\", 29000, 23, 'test');\r\nINSERT INTO employees VALUES (\"Chloe\", \"Engineering\", 23000, 25, 'test');\r\n\r\n# Execute WMA with basic option:\r\n\r\nsource=employees | trendline sort age wma(2, salary);\r\n\r\nname\tdept\tsalary\tage\tcon\tsalary_trendline\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\r\n\r\n\r\n# Execute WMA with alias:\r\n\r\nsource=employees | trendline sort age wma(2, salary) as CUSTOM_NAME\r\n\r\nname\tdept\tsalary\tage\tcon\tCUSTOM_NAME\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\r\n\r\n\r\n# Execute WMA with multiple calculations:\r\n\r\nsource=employees | trendline sort age wma(2, salary) as WMA_2 wma(3, salary) as WMA_3;\r\n\r\n\r\nname\tdept\tsalary\tage\tcon\tWMA_2\tWMA_3\r\nPaul\tEngineering\t29000\t23\ttest\tNULL\tNULL\r\nChloe\tEngineering\t23000\t25\ttest\t25000.0\tNULL\r\nJane\tMarketing\t29000\t28\ttest\t27000.0\t27000.0\r\nFred\tEngineering\t21000\t28\ttest\t23666.666666666668\t24000.0\r\nAlex\tSales------\t30000\t33\ttest\t27000.0\t26833.333333333332\r\nTom\tEngineering\t23000\t33\ttest\t25333.333333333332\t25000.0\r\nLisa\tSales------\t10000\t35\ttest\t14333.333333333334\t17666.666666666668\r\nJeff\tMarketing\t35000\t38\ttest\t26666.666666666668\t24666.666666666668\r\nEvan\tSales------\t32000\t38\ttest\t33000.0\t29333.333333333332\r\nTime taken: 0.466 seconds, Fetched 9 row(s)\r\n\r\n\r\n```\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@andy-k-improving please add DCO (sign-off)"
                },
                {
                    "user": "andy-k-improving",
                    "comment": "> @andy-k-improving please add DCO (sign-off)\r\n\r\nDone."
                },
                {
                    "user": "andy-k-improving",
                    "comment": "> @andy-k-improving\r\n> \r\n> ## please add the relevant [documentation](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/ppl-lang/ppl-trendline-command.md) references including [examples](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/ppl-lang/PPL-Example-Commands.md)\r\n\r\nDone."
                },
                {
                    "user": "andy-k-improving",
                    "comment": "@YANG-DB I have updated the example and documentation, would you mind to have another look?\r\n\r\nThanks, "
                }
            ],
            "owner": "andy-k-improving"
        },
        {
            "title": "[WIP] PPL geoip function pt. 2",
            "description": "Description\r\nPPL geoip function\r\n\r\nIssues Resolved\r\nhttps://github.com/opensearch-project/opensearch-spark/issues/672\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n\r\nThis PR is a continuation of https://github.com/opensearch-project/opensearch-spark/pull/781 due to lacking permissions to push to forked branch in said PR\r\n",
            "comments": [
                {
                    "user": "kenrickyap",
                    "comment": "Hi @YANG-DB, I heard from Anas that you had a method of implementing ip2geo functionality for Spark. I wanted to check with you that our current approach aligns with your method.\r\n\r\nCurrent Plan:\r\nLeveraging [SerializableUdf](https://github.com/opensearch-project/opensearch-spark/pull/706/files#diff-4fc2e6dc7264b8eac89050f1f19c51381f61fa36b302d78bcc0bc260498845fe) create a UDF that does the follow:\r\n\r\n1. Check if in-memory cache object for datasource exists.\r\n2. If cache object does not exists create new in-memory cache object from csv retrieved from datasource manifest. (manifest to CsvParser logic can be stripped from geospatial ip2geo).\r\n4. Search cached object for GeoIP data.\r\n5. Return GeoIP data.\r\n\r\nThis PR has a stub udf implementation for better idea of how this would be implemented.\r\n\r\nAll of this would have to be implemented within the Spark library, as currently I am not aware of how to access any geospatial artifacts. If you know of a better way to implement ip2geo please let me know! Thanks!"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@kenrickyap please update the DCO (contributor sign-off) \r\n"
                },
                {
                    "user": "kenrickyap",
                    "comment": "**Flow diagram for ip2geo UDF**\r\n\r\n![mermaid-diagram-2024-11-06-140042](https://github.com/user-attachments/assets/a7d48707-0348-4558-9b45-8a51a353d1d8)\r\n\r\n**Design Details**\r\n\r\n- Ip2GeoCache will be a Gauva Cache that has datasource string as key and CidrTree as value\r\n- CidrTree will be Trie (will use apache.common PatriciaTrie as I see that apache.common is already included in project)\r\n  - Am using trie instead of map as it is more well suited for longest prefix matching task, and this task is similar to ip to cidr matching\r\n  - each CidrTreeNode will store:\r\n    - nth bit value of cidr (n is the depth of tree)\r\n    - geo_data if there is matching cidr row in datasource csv\r\n    - child CidrTreeNodes\r\n - Will retrieve CsvParser from manifest using similar methodology as in geospatial [ip2geo](https://github.com/opensearch-project/geospatial/blob/main/src/main/java/org/opensearch/geospatial/ip2geo/dao/GeoIpDataDao.java#L188)\r\n \r\n **Pros**\r\n \r\n - ip2geo functionality is achieved.\r\n - implementation is simple and does not depend on any additional libraries that do not already exist in the project.\r\n \r\n **Cons**\r\n \r\n - calculations are done in-memory as a UDF, this means that multiple instances of Ip2GeoCache will be created in distributed Spark systems and they will not sync.\r\n - not leveraging job-scheduler to run ip2geo task and not leveraging OpenSearch to store ip2geo data as geospatial does."
                },
                {
                    "user": "YANG-DB",
                    "comment": "> **Flow diagram for ip2geo UDF**\r\n> \r\n> ![mermaid-diagram-2024-11-06-140042](https://private-user-images.githubusercontent.com/121634635/383740205-a7d48707-0348-4558-9b45-8a51a353d1d8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA5MzcxNTksIm5iZiI6MTczMDkzNjg1OSwicGF0aCI6Ii8xMjE2MzQ2MzUvMzgzNzQwMjA1LWE3ZDQ4NzA3LTAzNDgtNDU1OC05YjQ1LThhNTFhMzUzZDFkOC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwNlQyMzQ3MzlaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03ZGE4ZTM3NzE5NDU1Y2NlOWNhNjVhNDFjOWFiZDBiZWJjZWIzOWRkYzNmMDllMmM5MmFhY2Y0ZDJmNzZkNjI1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.xQmnmdwvtN2M2Dt0INzk-OtazcQRuErwn7ewiaKvRGg)\r\n> \r\n> **Design Details**\r\n> \r\n> * Ip2GeoCache will be a Gauva Cache that has datasource string as key and CidrTree as value\r\n> * CidrTree will be Trie (will use apache.common PatriciaTrie as I see that apache.common is already included in project)\r\n>   \r\n>   * Am using trie instead of map as it is more well suited for longest prefix matching task, and this task is similar to ip to cidr matching\r\n>   * each CidrTreeNode will store:\r\n>     \r\n>     * nth bit value of cidr (n is the depth of tree)\r\n>     * geo_data if there is matching cidr row in datasource csv\r\n>     * child CidrTreeNodes\r\n> * Will retrieve CsvParser from manifest using similar methodology as in geospatial [ip2geo](https://github.com/opensearch-project/geospatial/blob/main/src/main/java/org/opensearch/geospatial/ip2geo/dao/GeoIpDataDao.java#L188)\r\n> \r\n> **Pros**\r\n> \r\n> * ip2geo functionality is achieved.\r\n> * implementation is simple and does not depend on any additional libraries that do not already exist in the project.\r\n> \r\n> **Cons**\r\n> \r\n> * calculations are done in-memory as a UDF, this means that multiple instances of Ip2GeoCache will be created in distributed Spark systems and they will not sync.\r\n> * not leveraging job-scheduler to run ip2geo task and not leveraging OpenSearch to store ip2geo data as geospatial does.\r\n\r\n@kenrickyap a few questions here:\r\n- how / is the Ip2GeoCache memory shared between sessions ? is it a signleton ? \r\n- plz also add the use case where the geo_data_source will be a table / index (in OpenSearch) / service (API) - lets create a general purpose facade for this to hide different datasource drivers\r\n- I would like to see a more detailed description of the `CidrTreeNode` including an example and simple high level explanations (the geo tree diagram ?)\r\n- explain why is it worth while to add this trie-tree instead of using a hash-map / search ?\r\n- can u also add a pseudo-code here (in the issue) to clarity the composition ?\r\n"
                },
                {
                    "user": "kenrickyap",
                    "comment": "> @kenrickyap a few questions here:\r\n> \r\n> * how / is the Ip2GeoCache memory shared between sessions ? is it a signleton ?\r\n\r\nYes we will create Ip2GeoCache to be a singleton. From my understanding this should allow the cache to be accessible between sessions \r\n\r\n> * plz also add the use case where the geo_data_source will be a table / index (in OpenSearch) / service (API) - lets create a general purpose facade for this to hide different datasource drivers\r\n\r\nWill update the flow diagram to reflect facade for different datasource. \r\n\r\nHowever, would it be possible to provide a service (API) example usage? I am not to sure what such a datasource would be expected to return. \r\n\r\nAlso would there be a fixed schema for an index in opensource?\r\n\r\n> * I would like to see a more detailed description of the `CidrTreeNode` including an example and simple high level explanations (the geo tree diagram ?)\r\n\r\nIn hindsight I will just use a hashmap to store cidr geo_data where the cidr bitstring will be the key and geo_data will be a value. To preform the ip cidr matching will implement lookup function to convert ip to bitstring and reduce bit length till key is found in map.\r\n\r\nInitially I wanted to leverage the `prefixMap` function as I thought this would find the best fitting cidr mask for a given ip in O(1), which would mean I wouldn't have to implement my own lookup function. However as I was trying implement this noticed that `prefixMap` takes a prefix and finds all keys that have the prefix, which is the opposite with what I want.\r\n\r\n> * explain why is it worth while to add this trie-tree instead of using a hash-map / search ?\r\n\r\nAs mentioned above will use a hash-map instead of trie.\r\n\r\n> * can u also add a pseudo-code here (in the issue) to clarity the composition ?\r\n\r\nWill have pseudo-code and code added by EOD"
                }
            ],
            "owner": "kenrickyap"
        },
        {
            "title": "Add metrics for successful/failed Spark index creation",
            "description": "### Description\r\nSuccess and failure metrics for spark index creation -- note that \"failure\" here means an execution failure (e.g. Spark couldn't execute the query), not validation failure (e.g. the index already exists).\r\n\r\nE.g. success for creating a skipping index:\r\n<img width=\"1170\" alt=\"image\" src=\"https://github.com/user-attachments/assets/856d9ecb-7abc-4ecf-9d1f-f713226c660a\">\r\n\r\n### Related Issues\r\nN/A\r\n\r\n### Check List\r\n- [ ] Updated documentation (docs/ppl-lang/README.md)\r\n- [ ] Implemented unit tests\r\n- [ ] Implemented tests for combination with other commands\r\n- [X] New added source code should include a copyright header\r\n- [X] Commits are signed per the DCO using `--signoff`\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/sql/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "Swiddis"
        },
        {
            "title": "PPL geoip function",
            "description": "### Description\r\nPPL geoip function\r\n\r\n### Issues Resolved\r\nhttps://github.com/opensearch-project/opensearch-spark/issues/672\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "salyh",
                    "comment": "@YANG-DB @penghuo @lukasz-soszynski-eliatra @kt-eliatra @dr-lilienthal PPL geoip function syntax proposal\r\n\r\nPlease review and comment if needed"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@salyh thanks\r\non my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource,  can/should the `datasource` be one of the following:\r\n - external API\r\n - table\r\n - file \r\n Lets add these capabilities ..."
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin @penghuo @dai-chen please add u'r comments here"
                },
                {
                    "user": "salyh",
                    "comment": "> @salyh thanks on my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource, can/should the `datasource` be one of the following:\r\n> \r\n>     * external API\r\n> \r\n>     * table\r\n> \r\n>     * file\r\n>       Lets add these capabilities ...\r\n\r\n@YANG-DB @LantaoJin @penghuo @dai-chen \r\n\r\ndatasource refers to the datasources provided by the ip2geo processor https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/\r\n\r\nMy idea was to leveage the already present capabilities of the [ip2geo processor](https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/) and extend it as necessary. Because it would avoid code duplication. When I read the docs of the processor correct, GeoLite2 is supported by now. I propose to add the above requested functionality to the processor. Then we try to use the code from the processor (as a library) to solve this issue. Otherwise we would reinvent the wheel.\r\n\r\nAnd there is also https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/geoip/ in data prepper which could be facilitated"
                },
                {
                    "user": "YANG-DB",
                    "comment": "> > @salyh thanks on my end looking good - only question is the `datasource` definition - how does the user creates a new `datasource, can/should the `datasource` be one of the following:\r\n> > ```\r\n> > * external API\r\n> > \r\n> > * table\r\n> > \r\n> > * file\r\n> >   Lets add these capabilities ...\r\n> > ```\r\n> \r\n> @YANG-DB @LantaoJin @penghuo @dai-chen\r\n> \r\n> datasource refers to the datasources provided by the ip2geo processor https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/\r\n> \r\n> My idea was to leveage the already present capabilities of the [ip2geo processor](https://opensearch.org/docs/latest/ingest-pipelines/processors/ip2geo/) and extend it as necessary. Because it would avoid code duplication. When I read the docs of the processor correct, GeoLite2 is supported by now. I propose to add the above requested functionality to the processor. Then we try to use the code from the processor (as a library) to solve this issue. Otherwise we would reinvent the wheel.\r\n> \r\n> And there is also https://opensearch.org/docs/latest/data-prepper/pipelines/configuration/processors/geoip/ in data prepper which could be facilitated\r\n\r\n@salyh this sounds like a good idea !\r\n@LantaoJin @penghuo - any comments ?"
                },
                {
                    "user": "kenrickyap",
                    "comment": "HI @salyh,\r\n\r\nCurrently looking into implementation if this feature. I saw above conversation on planning to leverage ip2geo functionality within geospatial plugin. Did you have a clear idea on how this can be done?\r\n\r\nFrom my understanding to leverage ip2geo, one must first create a ip2geo processor then attach it to a opensearch pipeline. I am not too sure how the Spark plugin could access this functionality. Some pointers would be much appreciated :)\r\n\r\nThanks!"
                },
                {
                    "user": "salyh",
                    "comment": "> HI @salyh,\r\n> \r\n> Currently looking into implementation if this feature. I saw above conversation on planning to leverage ip2geo functionality within geospatial plugin. Did you have a clear idea on how this can be done?\r\n> \r\n> From my understanding to leverage ip2geo, one must first create a ip2geo processor then attach it to a opensearch pipeline. I am not too sure how the Spark plugin could access this functionality. Some pointers would be much appreciated :)\r\n> \r\n> Thanks!\r\n\r\nThe idea is not to leverage the processor as a processor but to reuse its code (either by copying it - not ideal - , or by separating them into a kind of shared commons library or by adding them as a dependency on code level)."
                },
                {
                    "user": "kenrickyap",
                    "comment": "Sounds good my current idea is to create an spi for the plugin, as was done for job-scheduler (allowing the geospatial plugin to implement ExtensiblePlugin) am still working on a design. This is to allow the sql plugin to access the ip2geo functionality (am also working on the iplocation functionality there).\r\n\r\nWould you know if the Spark project would be able to leverage the SPI since it is not a plugin? If we are to expose the ip2geo functionality I would rather create something that works both for the sql-plugin and Spark."
                },
                {
                    "user": "kenrickyap",
                    "comment": "Hi @salyh, spent a bit more time investigating this ticket. currently plan is to rewrite the geo2ip functionality within the spark project. The reason for this is that currently the geo2ip processor with the geospatial plugin is intwined with using the job scheduler extension. From my understanding there is no way to leverage usage of this extension in spark so a lot of the functionality would not be able to be migrated.\r\n\r\nThe rewritten geo2ip within spark will:\r\n- create spark temp table storing data from geoip datasource if table does not already exist\r\n- retrieve information from said table based on provided ips.\r\n\r\nThe current design question is if it would be better to use spark UDF or reflection to call [java_method](https://spark.apache.org/docs/3.5.3/api/sql/index.html#java_method) to call the rewritten geo2ip functionality?"
                },
                {
                    "user": "kenrickyap",
                    "comment": "> Hi @salyh, spent a bit more time investigating this ticket. currently plan is to rewrite the geo2ip functionality within the spark project. The reason for this is that currently the geo2ip processor with the geospatial plugin is intwined with using the job scheduler extension. From my understanding there is no way to leverage usage of this extension in spark so a lot of the functionality would not be able to be migrated.\r\n> \r\n> The rewritten geo2ip within spark will:\r\n> \r\n> * create spark temp table storing data from geoip datasource if table does not already exist\r\n> * retrieve information from said table based on provided ips.\r\n> \r\n> The current design question is if it would be better to use spark UDF or reflection to call [java_method](https://spark.apache.org/docs/3.5.3/api/sql/index.html#java_method) to call the rewritten geo2ip functionality?\r\n\r\nWill follow CIDR https://github.com/opensearch-project/opensearch-spark/pull/706/files approach and use UDF"
                }
            ],
            "owner": "salyh"
        },
        {
            "title": "Ppl help command",
            "description": "### Description\r\nAdd PPL `-help` command to assist grammar explaining and assisting users on different dialects and semantic\r\n\r\n```sql\r\nsearch -help\r\n```\r\n\r\nresults in:\r\n```\r\nSEARCH Command:\r\n\r\nSyntax:\r\n   (SEARCH)? fromClause\r\n   | (SEARCH)? fromClause logicalExpression\r\n   | (SEARCH)? logicalExpression fromClause\r\n\r\nDescription:\r\nThe SEARCH command is used to retrieve data from a specified source. It can be used with or without additional filters.\r\n- You can specify the data source using the FROM clause.\r\n- You can add filters using logical expressions.\r\n- The order of FROM clause and logical expression can be interchanged.\r\n\r\n```\r\n\r\n### Issues Resolved\r\n- https://github.com/opensearch-project/opensearch-spark/issues/658\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin \r\nI would appreciate your feedback on the approach i've drafter here for adding `help` parameter attribute for each command, for example the following would provide information and examples for the `search` command:\r\n```sql\r\nsearch -help\r\n```\r\nThis would be done for all commands we want to add the `help` description to.\r\n\r\n**Implementation concepts and consideration** \r\n - Added a dedicated `AstCommandDescriptionVisitor` which is an additional visitor (to the ASTBuilder ) that is only responsible of the commands decoration visiting.\r\n - The result is that `AstStatementBuilder` now hold a chain of visitors that run on each query and produce a plan (or not if they dont apply)\r\n - `FlintPPLSparkExtensions` was added with `injectPlannerStrategy` to override the standard queries in case of a `PrintLiteralCommandDescriptionLogicalPlan` plan and to avoid reaching to the underlying SQL engine and responding with the actual planner's text that was generated during the former stage\r\n\r\nPlease let me know if this sounds reasonable and if you have any suggestions to make this simpler\r\nThanks"
                },
                {
                    "user": "LantaoJin",
                    "comment": "Hi @YANG-DB , I haven't get the point what is the `help` command for. \r\n> The result is that AstStatementBuilder now hold a chain of visitors that run on each query and produce a plan (or not if they dont apply)\r\n\r\nSeems you are going to add a `debug` or `explain` command to display the translated Spark unresolved plan tree. `help` sounds like a customer towards command to show usage of PPL. \r\n\r\nIf your proposal is for `debug` or `explain` purpose, I think the display info not only includes the unresolved plan, but also the physical plan (which will reach to the underlying SQL engine) with a trigger.\r\n\r\nWhat confused me is the usage in description. It definitely is a `help` command. Beside the syntax, we can add some usage or examples.\r\n\r\nFor the `debug` or `explain` command, we need to display a generated plan for the PPL query user provided.\r\n```sql\r\nSEARCH explain=index\r\n| <ppl command1>\r\n| <ppl command2\r\n```\r\nor\r\n```sql\r\nEXPLAIN source=index\r\n| <ppl command1>\r\n| <ppl command2\r\n```"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@YANG-DB plz resolve conflict"
                }
            ],
            "owner": "YANG-DB"
        },
        {
            "title": "Add `INPUT` command to load a CSV file",
            "description": "### Description\r\nThis PR is for the syntax **option A** described in https://github.com/opensearch-project/opensearch-spark/issues/638\r\nThe PR for **option B** is https://github.com/opensearch-project/opensearch-spark/pull/678.\r\n\r\n#### Syntax of new command `input` (option A):\r\n```\r\ninput <fileUrl>\r\n```\r\nUsage:\r\n```\r\ninput \"s3://bucket_name/folder1/folder2/flights.csv\"\r\n```\r\n\r\n### Issues Resolved\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/638\r\n\r\n### Check List\r\n- [x] Updated documentation (ppl-spark-integration/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] Commits are signed per the DCO using --signoff \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "LantaoJin",
                    "comment": "@YANG-DB Which one do you think is better? #638 vs #678\r\nThe CI failed cause by\r\n> [PATH_NOT_FOUND] Path does not exist: file:/home/runner/work/**opensearch-spark/opensearch-spark/integ-test/integ-test**/src/integration/resources/opensearch_dashboards_sample_data_flights.csv.\r\n\r\nDo you know why the resources path contains duplicated folders in our github build node? The actual csv file located in $PROJECT/integ-test/src/integration/resources/opensearch_dashboards_sample_data_flights.csv\r\n"
                },
                {
                    "user": "YANG-DB",
                    "comment": "Let's keep this - option A #638 version as the one we go with "
                },
                {
                    "user": "YANG-DB",
                    "comment": "> @YANG-DB Which one do you think is better? #638 vs #678 The CI failed cause by\r\n> \r\n> > [PATH_NOT_FOUND] Path does not exist: file:/home/runner/work/**opensearch-spark/opensearch-spark/integ-test/integ-test**/src/integration/resources/opensearch_dashboards_sample_data_flights.csv.\r\n> \r\n> Do you know why the resources path contains duplicated folders in our github build node? The actual csv file located in $PROJECT/integ-test/src/integration/resources/opensearch_dashboards_sample_data_flights.csv\r\n\r\nLet me try to investigate this today..."
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin can u also plz revolve latest merge conflicts ?\r\nthanks"
                },
                {
                    "user": "LantaoJin",
                    "comment": "> @LantaoJin can u also plz revolve latest merge conflicts ? thanks\r\n\r\n@YANG-DB  I need some time to double confirm the file loading only be executed at once. Let me see how to get the metrics from Spark side in IT suite. Will rebase the code when my checking done."
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin can u plz change to PR into `draft` mode ?"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin \r\nplease add the needed documentation in the following locations:\r\n - [ppl commands list](https://github.com/opensearch-project/opensearch-spark/tree/main/docs/ppl-lang)\r\n - [ppl functions list](https://github.com/opensearch-project/opensearch-spark/tree/main/docs/ppl-lang/functions)\r\n - [ppl examples doc](https://github.com/opensearch-project/opensearch-spark/blob/main/docs/ppl-lang/PPL-Example-Commands.md)"
                }
            ],
            "owner": "LantaoJin"
        },
        {
            "title": "Support FileSourceRelation to load CSV in PPL",
            "description": "### Description\r\nThis PR is for the syntax **option B** described in https://github.com/opensearch-project/opensearch-spark/issues/638\r\nThe PR for **option A** is https://github.com/opensearch-project/opensearch-spark/pull/678.\r\n\r\n#### Syntax of option B\r\n```\r\nsearch file=<tempTableName> <fileUrl> [predicate]\r\n```\r\nUsage:\r\n```\r\nsearch file=table1 \"s3://bucket_name/folder1/folder2/flights.csv\" FlightDelay > 500\r\nsearch file=table1 \"s3://bucket_name/folder1/folder2/flights.csv.gz\" FlightDelay > 500\r\nsearch file=table1 \"s3a://bucket_name/folder1/folder2/flights.parquet\" FlightDelay > 500\r\n```\r\n\r\nOnly single file is supported now.\r\n\r\nPS: the current `search` command syntax is\r\n```\r\nsearch index=<indexName> [predicate]\r\nsearch source=<indexName> [predicate]\r\n```\r\n\r\n### Issues Resolved\r\nResolves https://github.com/opensearch-project/opensearch-spark/issues/638\r\n\r\n### Check List\r\n- [x] Updated documentation (ppl-spark-integration/README.md)\r\n- [x] Implemented unit tests\r\n- [x] Implemented tests for combination with other commands\r\n- [x] Commits are signed per the DCO using --signoff \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin I would go with option A as shown in #678 \r\nThanks again for u'r support and contributions"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin can u resolve the latest conflict plz ?"
                },
                {
                    "user": "YANG-DB",
                    "comment": "@LantaoJin can u plz change to PR into `draft` mode ?"
                }
            ],
            "owner": "LantaoJin"
        },
        {
            "title": "Disallow queries that reference catalog views",
            "description": "### Description\r\nDisallow queries that reference catalog views\r\n\r\n### Issues Resolved\r\n#534  \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "dai-chen",
                    "comment": "As @penghuo mentioned above, the changes are not directly related to Flint but it will disallow any SQL view created in Flint? I assume this is actually not the requirement for Flint Spark extension. I thought we just want to disable Glue Catalog View which may have the issue reported."
                },
                {
                    "user": "eirsep",
                    "comment": "> As @penghuo mentioned above, the changes are not directly related to Flint but it will disallow any SQL view created in Flint? I assume this is actually not the requirement for Flint Spark extension. I thought we just want to disable Glue Catalog View which may have the issue reported.\r\n\r\n@dai-chen What additional check can I add to ensure the View is actually a Glue catalog view?"
                }
            ],
            "owner": "eirsep"
        },
        {
            "title": "Add Docker-compose setting for IT testing",
            "description": "### Description\r\nAdd Docker-compose setting for IT testing\r\n\r\n### Issues Resolved\r\nhttps://github.com/opensearch-project/opensearch-spark/issues/601\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "salyh",
                    "comment": "I need still to wire spark and opensearch and add some docs ..."
                },
                {
                    "user": "YANG-DB",
                    "comment": "nice\r\ncan you also add opensearch dashbords to the docker compose so that we can acutally test ppl directly from the query workbench ?\r\nthanks ...\r\nsee example [here](https://github.com/opensearch-project/opentelemetry-demo/blob/4108ea8918dac81a97234a9181b01d4334429def/docker-compose.yml#L827)"
                },
                {
                    "user": "salyh",
                    "comment": "Opensearch Dashboards are already there, see https://github.com/opensearch-project/opensearch-spark/pull/606/files#r1736775479"
                },
                {
                    "user": "salyh",
                    "comment": "Next steps:\r\n\r\n* Triage https://github.com/opensearch-project/documentation-website/issues/8212\r\n* Check if Livy + Spark is a workaround (that would need aLivy client in OpenSearch-SQL plugin)\r\n* Discuss if there are other solutions together with @penghuo and @YANG-DB \r\n* Decide to keep the PR or not"
                },
                {
                    "user": "salyh",
                    "comment": "@YANG-DB any update?"
                }
            ],
            "owner": "salyh"
        },
        {
            "title": "Create dataframe from list instead of RDD",
            "description": "### Description\r\nCreate dataframe from list instead of RDD, since EMR-S does not support create dataframe from RDD. \r\n\r\n### Issues Resolved\r\nn/a\r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [
                {
                    "user": "penghuo",
                    "comment": "To maintainer, Please don't merge this PR until we get confirmation that EMR-S upgrade is required."
                }
            ],
            "owner": "penghuo"
        },
        {
            "title": "Fix build index with special full table name",
            "description": "### Description\r\nFix build index with special full table name, including special characters in catalog name and database name.\r\nAlso fix test suite for it.\r\n\r\nNote: removed a test case that was added unintentionally\r\n\r\n### Issues Resolved\r\n_List any issues this PR will resolve, e.g. Closes [...]._ \r\n\r\nBy submitting this pull request, I confirm that my contribution is made under the terms of the Apache 2.0 license.\r\nFor more information on following Developer Certificate of Origin and signing off your commits, please check [here](https://github.com/opensearch-project/OpenSearch/blob/main/CONTRIBUTING.md#developer-certificate-of-origin).\r\n",
            "comments": [],
            "owner": "seankao-az"
        }
    ],
    "source_code": [
        {
            "file_name": ".gitignore",
            "comments": [
                "# IDE files",
                ".idea/",
                "*.iml",
                "",
                "# Compiled output",
                "target/",
                "project/target/",
                "",
                "# Log files",
                "logs/",
                "",
                "# sbt-specific files",
                ".sbtserver",
                ".sbt/",
                ".bsp/",
                "",
                "# Miscellaneous",
                ".DS_Store",
                "*.class",
                "*.log",
                "*.zip",
                "",
                "gen/",
                ""
            ]
        },
        {
            "file_name": ".lychee.excludes",
            "comments": [
                "https://aws.oss.sonatype.*",
                "http://localhost.*",
                "https://localhost.*",
                "https://odfe-node1:9200/.*",
                "https://community.tableau.com/docs/DOC-17978",
                ".*family.zzz",
                "https://pypi.python.org/pypi/opensearchsql/.*",
                "opensearch.*",
                ".*@amazon.com",
                ".*email.com",
                ".*git@github.com.*",
                "http://timestamp.verisign.com/scripts/timstamp.dll.*",
                ".*\\/PowerBIConnector\\/bin\\/Release"
            ]
        },
        {
            "file_name": ".scalafmt.conf",
            "comments": [
                "version = 2.7.5"
            ]
        },
        {
            "file_name": "CODE_OF_CONDUCT.md",
            "comments": [
                "## Code of Conduct",
                "This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).",
                "For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact",
                "opensource-codeofconduct@amazon.com with any additional questions or comments.",
                ""
            ]
        },
        {
            "file_name": "CONTRIBUTING.md",
            "comments": [
                "# Contributing Guidelines",
                "",
                "Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional",
                "documentation, we greatly value feedback and contributions from our community.",
                "",
                "Please read through this document before submitting any issues or pull requests to ensure we have all the necessary",
                "information to effectively respond to your bug report or contribution.",
                "",
                "",
                "## Reporting Bugs/Feature Requests",
                "",
                "We welcome you to use the GitHub issue tracker to report bugs or suggest features.",
                "",
                "When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already",
                "reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:",
                "",
                "* A reproducible test case or series of steps",
                "* The version of our code being used",
                "* Any modifications you've made relevant to the bug",
                "* Anything unusual about your environment or deployment",
                "",
                "",
                "## Contributing via Pull Requests",
                "Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:",
                "",
                "1. You are working against the latest source on the *main* branch.",
                "2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.",
                "3. You open an issue to discuss any significant work - we would hate for your time to be wasted.",
                "",
                "To send us a pull request, please:",
                "",
                "1. Fork the repository.",
                "2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.",
                "3. Ensure local tests pass.",
                "4. Commit to your fork using clear commit messages.",
                "5. Send us a pull request, answering any default questions in the pull request interface.",
                "6. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.",
                "",
                "GitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and",
                "[creating a pull request](https://help.github.com/articles/creating-a-pull-request/).",
                "",
                "",
                "## Finding contributions to work on",
                "Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.",
                "",
                "",
                "## Code of Conduct",
                "This project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).",
                "For more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact",
                "opensource-codeofconduct@amazon.com with any additional questions or comments.",
                "",
                "",
                "## Security issue notifications",
                "If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.",
                "",
                "",
                "## Licensing",
                "",
                "See the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.",
                ""
            ]
        },
        {
            "file_name": "DEVELOPER_GUIDE.md",
            "comments": [
                "# Developer Guide",
                "",
                "## Package",
                "If you want to package the single jar for, you can do so by running the following command:",
                "```",
                "sbt assembly",
                "```",
                "",
                "## Unit Test",
                "To execute the unit tests, run the following command:",
                "```",
                "sbt test",
                "```",
                "",
                "## Integration Test",
                "The integration test is defined in the `integration` directory of the project. The integration tests will automatically trigger unit tests and will only run if all unit tests pass. If you want to run the integration test for the project, you can do so by running the following command:",
                "```",
                "sbt integtest/integration",
                "```",
                "If you get integration test failures with error message \"Previous attempts to find a Docker environment failed\" in macOS, fix the issue by following the checklist:",
                "1. Check you've installed Docker in your dev host. If not, install Docker first.",
                "2. Check if the file /var/run/docker.sock exists. If not, go to `3`.",
                "3. Run `sudo ln -s $HOME/.docker/desktop/docker.sock /var/run/docker.sock` or `sudo ln -s $HOME/.docker/run/docker.sock /var/run/docker.sock`",
                "4. If you use Docker Desktop, as an alternative of `3`, check mark the \"Allow the default Docker socket to be used (requires password)\" in advanced settings of Docker Desktop.",
                "",
                "### AWS Integration Test",
                "The `aws-integration` folder contains tests for cloud server providers. For instance, test against AWS OpenSearch domain, configure the following settings. The client will use the default credential provider to access the AWS OpenSearch domain.",
                "```",
                "export AWS_OPENSEARCH_HOST=search-xxx.us-west-2.on.aws",
                "export AWS_OPENSEARCH_SERVERLESS_HOST=xxx.us-west-2.aoss.amazonaws.com",
                "export AWS_REGION=us-west-2",
                "export AWS_EMRS_APPID=xxx",
                "export AWS_EMRS_EXECUTION_ROLE=xxx",
                "export AWS_S3_CODE_BUCKET=xxx",
                "export AWS_S3_CODE_PREFIX=xxx",
                "export AWS_OPENSEARCH_RESULT_INDEX=query_execution_result_glue",
                "```",
                "And run the following command:",
                "```",
                "sbt integtest/awsIntegration",
                "",
                "[info] AWSOpenSearchAccessTestSuite:",
                "[info] - should Create Pit on AWS OpenSearch",
                "[info] Run completed in 3 seconds, 116 milliseconds.",
                "[info] Total number of tests run: 1",
                "[info] Suites: completed 1, aborted 0",
                "[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0",
                "[info] All tests passed.",
                "```",
                "",
                "## Scala Formatting Guidelines",
                "",
                "For Scala code, flint use [spark scalastyle](https://github.com/apache/spark/blob/master/scalastyle-config.xml). Before submitting the PR, ",
                "make sure to use \"scalafmtAll\" to format the code. read more in [scalafmt sbt](https://scalameta.org/scalafmt/docs/installation.html#sbt)",
                "```",
                "sbt scalafmtAll",
                "```",
                "The code style is automatically checked, but users can also manually check it.",
                "```",
                "sbt scalastyle",
                "```",
                "For IntelliJ user, read more in [scalafmt IntelliJ](https://scalameta.org/scalafmt/docs/installation.html#intellij) to integrate ",
                "scalafmt with IntelliJ",
                ""
            ]
        },
        {
            "file_name": "LICENSE",
            "comments": [
                "",
                "                                 Apache License",
                "                           Version 2.0, January 2004",
                "                        http://www.apache.org/licenses/",
                "",
                "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION",
                "",
                "   1. Definitions.",
                "",
                "      \"License\" shall mean the terms and conditions for use, reproduction,",
                "      and distribution as defined by Sections 1 through 9 of this document.",
                "",
                "      \"Licensor\" shall mean the copyright owner or entity authorized by",
                "      the copyright owner that is granting the License.",
                "",
                "      \"Legal Entity\" shall mean the union of the acting entity and all",
                "      other entities that control, are controlled by, or are under common",
                "      control with that entity. For the purposes of this definition,",
                "      \"control\" means (i) the power, direct or indirect, to cause the",
                "      direction or management of such entity, whether by contract or",
                "      otherwise, or (ii) ownership of fifty percent (50%) or more of the",
                "      outstanding shares, or (iii) beneficial ownership of such entity.",
                "",
                "      \"You\" (or \"Your\") shall mean an individual or Legal Entity",
                "      exercising permissions granted by this License.",
                "",
                "      \"Source\" form shall mean the preferred form for making modifications,",
                "      including but not limited to software source code, documentation",
                "      source, and configuration files.",
                "",
                "      \"Object\" form shall mean any form resulting from mechanical",
                "      transformation or translation of a Source form, including but",
                "      not limited to compiled object code, generated documentation,",
                "      and conversions to other media types.",
                "",
                "      \"Work\" shall mean the work of authorship, whether in Source or",
                "      Object form, made available under the License, as indicated by a",
                "      copyright notice that is included in or attached to the work",
                "      (an example is provided in the Appendix below).",
                "",
                "      \"Derivative Works\" shall mean any work, whether in Source or Object",
                "      form, that is based on (or derived from) the Work and for which the",
                "      editorial revisions, annotations, elaborations, or other modifications",
                "      represent, as a whole, an original work of authorship. For the purposes",
                "      of this License, Derivative Works shall not include works that remain",
                "      separable from, or merely link (or bind by name) to the interfaces of,",
                "      the Work and Derivative Works thereof.",
                "",
                "      \"Contribution\" shall mean any work of authorship, including",
                "      the original version of the Work and any modifications or additions",
                "      to that Work or Derivative Works thereof, that is intentionally",
                "      submitted to Licensor for inclusion in the Work by the copyright owner",
                "      or by an individual or Legal Entity authorized to submit on behalf of",
                "      the copyright owner. For the purposes of this definition, \"submitted\"",
                "      means any form of electronic, verbal, or written communication sent",
                "      to the Licensor or its representatives, including but not limited to",
                "      communication on electronic mailing lists, source code control systems,",
                "      and issue tracking systems that are managed by, or on behalf of, the",
                "      Licensor for the purpose of discussing and improving the Work, but",
                "      excluding communication that is conspicuously marked or otherwise",
                "      designated in writing by the copyright owner as \"Not a Contribution.\"",
                "",
                "      \"Contributor\" shall mean Licensor and any individual or Legal Entity",
                "      on behalf of whom a Contribution has been received by Licensor and",
                "      subsequently incorporated within the Work.",
                "",
                "   2. Grant of Copyright License. Subject to the terms and conditions of",
                "      this License, each Contributor hereby grants to You a perpetual,",
                "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable",
                "      copyright license to reproduce, prepare Derivative Works of,",
                "      publicly display, publicly perform, sublicense, and distribute the",
                "      Work and such Derivative Works in Source or Object form.",
                "",
                "   3. Grant of Patent License. Subject to the terms and conditions of",
                "      this License, each Contributor hereby grants to You a perpetual,",
                "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable",
                "      (except as stated in this section) patent license to make, have made,",
                "      use, offer to sell, sell, import, and otherwise transfer the Work,",
                "      where such license applies only to those patent claims licensable",
                "      by such Contributor that are necessarily infringed by their",
                "      Contribution(s) alone or by combination of their Contribution(s)",
                "      with the Work to which such Contribution(s) was submitted. If You",
                "      institute patent litigation against any entity (including a",
                "      cross-claim or counterclaim in a lawsuit) alleging that the Work",
                "      or a Contribution incorporated within the Work constitutes direct",
                "      or contributory patent infringement, then any patent licenses",
                "      granted to You under this License for that Work shall terminate",
                "      as of the date such litigation is filed.",
                "",
                "   4. Redistribution. You may reproduce and distribute copies of the",
                "      Work or Derivative Works thereof in any medium, with or without",
                "      modifications, and in Source or Object form, provided that You",
                "      meet the following conditions:",
                "",
                "      (a) You must give any other recipients of the Work or",
                "          Derivative Works a copy of this License; and",
                "",
                "      (b) You must cause any modified files to carry prominent notices",
                "          stating that You changed the files; and",
                "",
                "      (c) You must retain, in the Source form of any Derivative Works",
                "          that You distribute, all copyright, patent, trademark, and",
                "          attribution notices from the Source form of the Work,",
                "          excluding those notices that do not pertain to any part of",
                "          the Derivative Works; and",
                "",
                "      (d) If the Work includes a \"NOTICE\" text file as part of its",
                "          distribution, then any Derivative Works that You distribute must",
                "          include a readable copy of the attribution notices contained",
                "          within such NOTICE file, excluding those notices that do not",
                "          pertain to any part of the Derivative Works, in at least one",
                "          of the following places: within a NOTICE text file distributed",
                "          as part of the Derivative Works; within the Source form or",
                "          documentation, if provided along with the Derivative Works; or,",
                "          within a display generated by the Derivative Works, if and",
                "          wherever such third-party notices normally appear. The contents",
                "          of the NOTICE file are for informational purposes only and",
                "          do not modify the License. You may add Your own attribution",
                "          notices within Derivative Works that You distribute, alongside",
                "          or as an addendum to the NOTICE text from the Work, provided",
                "          that such additional attribution notices cannot be construed",
                "          as modifying the License.",
                "",
                "      You may add Your own copyright statement to Your modifications and",
                "      may provide additional or different license terms and conditions",
                "      for use, reproduction, or distribution of Your modifications, or",
                "      for any such Derivative Works as a whole, provided Your use,",
                "      reproduction, and distribution of the Work otherwise complies with",
                "      the conditions stated in this License.",
                "",
                "   5. Submission of Contributions. Unless You explicitly state otherwise,",
                "      any Contribution intentionally submitted for inclusion in the Work",
                "      by You to the Licensor shall be under the terms and conditions of",
                "      this License, without any additional terms or conditions.",
                "      Notwithstanding the above, nothing herein shall supersede or modify",
                "      the terms of any separate license agreement you may have executed",
                "      with Licensor regarding such Contributions.",
                "",
                "   6. Trademarks. This License does not grant permission to use the trade",
                "      names, trademarks, service marks, or product names of the Licensor,",
                "      except as required for reasonable and customary use in describing the",
                "      origin of the Work and reproducing the content of the NOTICE file.",
                "",
                "   7. Disclaimer of Warranty. Unless required by applicable law or",
                "      agreed to in writing, Licensor provides the Work (and each",
                "      Contributor provides its Contributions) on an \"AS IS\" BASIS,",
                "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or",
                "      implied, including, without limitation, any warranties or conditions",
                "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A",
                "      PARTICULAR PURPOSE. You are solely responsible for determining the",
                "      appropriateness of using or redistributing the Work and assume any",
                "      risks associated with Your exercise of permissions under this License.",
                "",
                "   8. Limitation of Liability. In no event and under no legal theory,",
                "      whether in tort (including negligence), contract, or otherwise,",
                "      unless required by applicable law (such as deliberate and grossly",
                "      negligent acts) or agreed to in writing, shall any Contributor be",
                "      liable to You for damages, including any direct, indirect, special,",
                "      incidental, or consequential damages of any character arising as a",
                "      result of this License or out of the use or inability to use the",
                "      Work (including but not limited to damages for loss of goodwill,",
                "      work stoppage, computer failure or malfunction, or any and all",
                "      other commercial damages or losses), even if such Contributor",
                "      has been advised of the possibility of such damages.",
                "",
                "   9. Accepting Warranty or Additional Liability. While redistributing",
                "      the Work or Derivative Works thereof, You may choose to offer,",
                "      and charge a fee for, acceptance of support, warranty, indemnity,",
                "      or other liability obligations and/or rights consistent with this",
                "      License. However, in accepting such obligations, You may act only",
                "      on Your own behalf and on Your sole responsibility, not on behalf",
                "      of any other Contributor, and only if You agree to indemnify,",
                "      defend, and hold each Contributor harmless for any liability",
                "      incurred by, or claims asserted against, such Contributor by reason",
                "      of your accepting any such warranty or additional liability.",
                ""
            ]
        },
        {
            "file_name": "MAINTAINERS.md",
            "comments": [
                "## Overview",
                "",
                "This document contains a list of maintainers in this repo. See [opensearch-project/.github/RESPONSIBILITIES.md](https://github.com/opensearch-project/.github/blob/main/RESPONSIBILITIES.md#maintainer-responsibilities) that explains what the role of maintainer means, what maintainers do in this and other repos, and how they should be doing it. If you're interested in contributing, and becoming a maintainer, see [CONTRIBUTING](CONTRIBUTING.md).",
                "",
                "## Current Maintainers",
                "",
                "| Maintainer      | GitHub ID                                       | Affiliation |",
                "|-----------------|-------------------------------------------------| ----------- |",
                "| Eric Wei        | [mengweieric](https://github.com/mengweieric)   | Amazon      |",
                "| Joshua Li       | [joshuali925](https://github.com/joshuali925)   | Amazon      |",
                "| Rupal Mahajan   | [rupal-bq](https://github.com/rupal-bq)         | Amazon      |",
                "| Chen Dai        | [dai-chen](https://github.com/dai-chen)         | Amazon      |",
                "| Vamsi Manohar   | [vamsi-amazon](https://github.com/vamsi-amazon) | Amazon      |",
                "| Peng Huo        | [penghuo](https://github.com/penghuo)           | Amazon      |",
                "| Lior Perry      | [YANG-DB](https://github.com/YANG-DB)           | Amazon      |",
                "| Sean Kao        | [seankao-az](https://github.com/seankao-az)     | Amazon      |",
                "| Anirudha Jadhav | [anirudha](https://github.com/anirudha)         | Amazon      |",
                "| Kaituo Li       | [kaituo](https://github.com/kaituo)             | Amazon      |",
                "| Louis Chu       | [noCharger](https://github.com/noCharger)       | Amazon      |",
                "| Lantao Jin      | [LantaoJin](https://github.com/LantaoJin)       | Amazon      |",
                "| Tomoyuki Morita | [ykmr1224](https://github.com/ykmr1224)         | Amazon      |",
                ""
            ]
        },
        {
            "file_name": "NOTICE",
            "comments": [
                "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.",
                ""
            ]
        },
        {
            "file_name": "OpenSearch-PPL-Command-Process.md",
            "comments": [
                "# OpenSearch PPL Command Development Process",
                "This document outlines the formal process for proposing and implementing new PPL commands or syntax changes in OpenSearch.",
                "",
                "## Phase 1: Proposal",
                "",
                "### 1.1 Create GitHub Issue",
                "",
                "Start by creating a new GitHub issue using the following [template](.github/ISSUE_TEMPLATE/ppl_command_request.md):",
                "```",
                "name: PPL Command request",
                "about: Request a new PPL command Or syntax change",
                "title: '[PPL-Lang]'",
                "labels: 'enhancement, untriaged'",
                "assignees: ''",
                "---",
                "",
                "**Is your feature request related to a problem?**",
                "A clear and concise description of what the PPL command/syntax change is about, why is it needed, e.g. _I'm always frustrated when [...]_",
                "",
                "**What solution would you like?**",
                "A clear and concise description of what you want to happen.",
                "- Add Example new / updated syntax",
                "- [Optional] Add suggested [ANTLR](https://www.antlr.org/) suggested grammar",
                "",
                "**Add Proposal Document**",
                "Under the [docs/planning](../../docs/ppl-lang/planning) folder add a dedicated page for your suggested command or syntax change",
                "",
                "See [ppl-fillnull-command.md](../../docs/ppl-lang/planning/ppl-fillnull-command.md) example",
                "",
                "**Do you have any additional context?**",
                "Add any other context or screenshots about the feature request here.",
                "```",
                "",
                "### 1.2 Create Planning Document PR",
                "Create a Pull Request that adds a new markdown file under the `docs/ppl-lang/planning folder`. This document should include:",
                "",
                "1) Command overview and motivation",
                "2) Detailed syntax specification ",
                "3) Example usage scenarios ",
                "4) Implementation considerations ",
                "5) Potential limitations or edge cases",
                "",
                "## Phase 2: Review and Approval",
                "",
                "1) Community members and maintainers review the proposal ",
                "2) Feedback is incorporated into the planning document / PR comments",
                "3) Proposal is either accepted, rejected, or sent back for revision",
                "",
                "## Phase 3: Experimental Implementation",
                "Once approved, the command enters the experimental phase:",
                "",
                "1) Create implementation PR with:",
                "    - Code changes",
                "    - Comprehensive test suite",
                "    - Documentation updates",
                "",
                "2) Code is clearly marked as experimental using appropriate annotations ",
                "3) Documentation indicates experimental status ",
                "4) Experimental features are disabled by default in production",
                "",
                "## Phase 4: Maturation",
                "During the experimental phase:",
                "",
                "1) Gather user feedback ",
                "2) Address issues and edge cases ",
                "3) Refine implementation and documentation ",
                "4) Regular review of usage and stability",
                "",
                "## Phase 5: Formal Integration",
                "When the command has matured:",
                "",
                "1) Create PR to remove experimental status",
                "2) Update all documentation to reflect stable status",
                "3) Ensure backward compatibility",
                "4) Merge into main PPL command set",
                "",
                "---",
                "",
                "## Best Practices",
                "",
                "* Follow existing PPL command patterns and conventions",
                "* Ensure comprehensive test coverage",
                "* Provide clear, detailed documentation with examples",
                "* Consider performance implications",
                "* Maintain backward compatibility when possible",
                "",
                "## Timeline Expectations",
                "",
                "* Proposal Review: 1-2 weeks",
                "* Experimental Phase: 1-3 months",
                "* Maturation to Formal Integration: Based on community feedback and stability",
                ""
            ]
        },
        {
            "file_name": "README.md",
            "comments": [
                "# OpenSearch Flint",
                "",
                "OpenSearch Flint is ... It consists of four modules:",
                "",
                "- `flint-core`: a module that contains Flint specification and client.",
                "- `flint-commons`: a module that provides a shared library of utilities and common functionalities, designed to easily extend Flint's capabilities.",
                "- `flint-spark-integration`: a module that provides Spark integration for Flint and derived dataset based on it.",
                "- `ppl-spark-integration`: a module that provides PPL query execution on top of Spark See [PPL repository](https://github.com/opensearch-project/piped-processing-language).",
                "",
                "## Documentation",
                "",
                "Please refer to the [Flint Index Reference Manual](./docs/index.md) for more information.",
                "",
                "### PPL-Language",
                "",
                "* For additional details on PPL commands, see [PPL Commands Docs](docs/ppl-lang/README.md)",
                "",
                "* For additional details on Spark PPL Architecture, see [PPL Architecture](docs/ppl-lang/PPL-on-Spark.md)",
                "",
                "* For additional details on Spark PPL commands project, see [PPL Project](https://github.com/orgs/opensearch-project/projects/214/views/2)",
                "",
                "## Prerequisites",
                "",
                "Version compatibility:",
                "",
                "| Flint version | JDK version | Spark version | Scala version | OpenSearch |",
                "|---------------|-------------|---------------|---------------|------------|",
                "| 0.1.0         | 11+         | 3.3.1         | 2.12.14       | 2.6+       |",
                "| 0.2.0         | 11+         | 3.3.1         | 2.12.14       | 2.6+       |",
                "| 0.3.0         | 11+         | 3.3.2         | 2.12.14       | 2.13+      |",
                "| 0.4.0         | 11+         | 3.3.2         | 2.12.14       | 2.13+      |",
                "| 0.5.0         | 11+         | 3.5.1         | 2.12.14       | 2.17+      |",
                "| 0.6.0         | 11+         | 3.5.1         | 2.12.14       | 2.17+      |",
                "| 0.7.0         | 11+         | 3.5.1         | 2.12.14       | 2.17+      |",
                "",
                "## Flint Extension Usage ",
                "",
                "To use this application, you can run Spark with Flint extension:",
                "",
                "```",
                "spark-sql --conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintSparkExtensions\"",
                "```",
                "",
                "## PPL Extension Usage",
                "",
                "To use PPL to Spark translation, you can run Spark with PPL extension:",
                "",
                "```",
                "spark-sql --conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintPPLSparkExtensions\"",
                "```",
                "",
                "### Running With both Extension ",
                "```",
                "spark-sql --conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintPPLSparkExtensions,org.opensearch.flint.spark.FlintSparkExtensions\"",
                "```",
                "",
                "## Build",
                "",
                "To build and run this application with Spark, you can run (requires Java 11):",
                "",
                "```",
                "sbt clean standaloneCosmetic/publishM2",
                "```",
                "then add org.opensearch:opensearch-spark-standalone_2.12 when run spark application, for example,",
                "```",
                "bin/spark-shell --packages \"org.opensearch:opensearch-spark-standalone_2.12:0.7.0-SNAPSHOT\" \\",
                "                --conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintSparkExtensions\" \\",
                "                --conf \"spark.sql.catalog.dev=org.apache.spark.opensearch.catalog.OpenSearchCatalog\"",
                "```",
                "",
                "### PPL Build & Run ",
                "",
                "To build and run this PPL in Spark, you can run (requires Java 11):",
                "",
                "```",
                "sbt clean sparkPPLCosmetic/publishM2",
                "```",
                "then add org.opensearch:opensearch-spark-ppl_2.12 when run spark application, for example,",
                "```",
                "bin/spark-shell --packages \"org.opensearch:opensearch-spark-ppl_2.12:0.7.0-SNAPSHOT\" \\",
                "                --conf \"spark.sql.extensions=org.opensearch.flint.spark.FlintPPLSparkExtensions\" \\",
                "                --conf \"spark.sql.catalog.dev=org.apache.spark.opensearch.catalog.OpenSearchCatalog\"",
                "",
                "```",
                "",
                "## Code of Conduct",
                "",
                "This project has adopted an [Open Source Code of Conduct](./CODE_OF_CONDUCT.md).",
                "",
                "## Security",
                "",
                "If you discover a potential security issue in this project we ask that you notify OpenSearch Security directly via email to security@opensearch.org. Please do **not** create a public GitHub issue.",
                "",
                "## License",
                "",
                "See the [LICENSE](./LICENSE.txt) file for our project's licensing. We will ask you to confirm the licensing of your contribution.",
                "",
                "## Copyright",
                "",
                "Copyright OpenSearch Contributors. See [NOTICE](./NOTICE) for details.",
                ""
            ]
        },
        {
            "file_name": "build.sbt",
            "comments": [
                "/*",
                " * Copyright OpenSearch Contributors",
                " * SPDX-License-Identifier: Apache-2.0",
                " */",
                "import Dependencies._",
                "",
                "lazy val scala212 = \"2.12.14\"",
                "lazy val sparkVersion = \"3.5.1\"",
                "// Spark jackson version. Spark jackson-module-scala strictly check the jackson-databind version should compatible",
                "// https://github.com/FasterXML/jackson-module-scala/blob/2.18/src/main/scala/com/fasterxml/jackson/module/scala/JacksonModule.scala#L59",
                "lazy val jacksonVersion = \"2.15.2\"",
                "",
                "// The transitive opensearch jackson-databind dependency version should align with Spark jackson databind dependency version.",
                "// Issue: https://github.com/opensearch-project/opensearch-spark/issues/442",
                "lazy val opensearchVersion = \"2.6.0\"",
                "lazy val opensearchMavenVersion = \"2.6.0.0\"",
                "lazy val icebergVersion = \"1.5.0\"",
                "",
                "val scalaMinorVersion = scala212.split(\"\\\\.\").take(2).mkString(\".\")",
                "val sparkMinorVersion = sparkVersion.split(\"\\\\.\").take(2).mkString(\".\")",
                "",
                "ThisBuild / organization := \"org.opensearch\"",
                "",
                "ThisBuild / version := \"0.7.0-SNAPSHOT\"",
                "",
                "ThisBuild / scalaVersion := scala212",
                "",
                "ThisBuild / scalafmtConfig := baseDirectory.value / \"dev/.scalafmt.conf\"",
                "",
                "/**",
                " * ScalaStyle configurations",
                " */",
                "ThisBuild / scalastyleConfig := baseDirectory.value / \"scalastyle-config.xml\"",
                "",
                "/**",
                " * Tests cannot be run in parallel since multiple Spark contexts cannot run in the same JVM",
                " */",
                "ThisBuild / Test / parallelExecution := false",
                "",
                "// Run as part of compile task.",
                "lazy val compileScalastyle = taskKey[Unit](\"compileScalastyle\")",
                "",
                "// Run as part of test task.",
                "lazy val testScalastyle = taskKey[Unit](\"testScalastyle\")",
                "",
                "",
                "",
                "lazy val commonSettings = Seq(",
                "  javacOptions ++= Seq(\"-source\", \"11\"),",
                "  Compile / compile / javacOptions ++= Seq(\"-target\", \"11\"),",
                "  // Scalastyle",
                "  scalastyleConfig := (ThisBuild / scalastyleConfig).value,",
                "  compileScalastyle := (Compile / scalastyle).toTask(\"\").value,",
                "  Compile / compile := ((Compile / compile) dependsOn compileScalastyle).value,",
                "  testScalastyle := (Test / scalastyle).toTask(\"\").value,",
                "  Test / test := ((Test / test) dependsOn testScalastyle).value,",
                "  dependencyOverrides ++= Seq(",
                "    \"com.fasterxml.jackson.core\" % \"jackson-core\" % jacksonVersion,",
                "    \"com.fasterxml.jackson.core\" % \"jackson-databind\" % jacksonVersion",
                "  ))",
                "",
                "// running `scalafmtAll` includes all subprojects under root",
                "lazy val root = (project in file(\".\"))",
                "  .aggregate(flintCommons, flintCore, flintSparkIntegration, pplSparkIntegration, sparkSqlApplication, integtest)",
                "  .disablePlugins(AssemblyPlugin)",
                "  .settings(name := \"flint\", publish / skip := true)",
                "",
                "lazy val flintCore = (project in file(\"flint-core\"))",
                "  .disablePlugins(AssemblyPlugin)",
                "  .dependsOn(flintCommons)",
                "  .settings(",
                "    commonSettings,",
                "    name := \"flint-core\",",
                "    scalaVersion := scala212,",
                "    libraryDependencies ++= Seq(",
                "      \"org.opensearch.client\" % \"opensearch-rest-client\" % opensearchVersion,",
                "      \"org.opensearch.client\" % \"opensearch-rest-high-level-client\" % opensearchVersion",
                "        exclude (\"org.apache.logging.log4j\", \"log4j-api\"),",
                "      \"org.opensearch.client\" % \"opensearch-java\" % opensearchVersion",
                "        // error: Scala module 2.13.4 requires Jackson Databind version >= 2.13.0 and < 2.14.0 -",
                "        // Found jackson-databind version 2.14.",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\")",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-core\")",
                "        exclude (\"org.apache.httpcomponents.client5\", \"httpclient5\"),",
                "      \"org.opensearch\" % \"opensearch-job-scheduler-spi\" % opensearchMavenVersion,",
                "      \"dev.failsafe\" % \"failsafe\" % \"3.3.2\",",
                "      \"com.amazonaws\" % \"aws-java-sdk\" % \"1.12.397\" % \"provided\"",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      \"com.amazonaws\" % \"aws-java-sdk-cloudwatch\" % \"1.12.593\"",
                "        exclude(\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      \"software.amazon.awssdk\" % \"auth-crt\" % \"2.28.10\",",
                "      \"org.projectlombok\" % \"lombok\" % \"1.18.30\" % \"provided\",",
                "      \"org.scalactic\" %% \"scalactic\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest-flatspec\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatestplus\" %% \"mockito-4-6\" % \"3.2.15.0\" % \"test\",",
                "      \"com.stephenn\" %% \"scalatest-json-jsonassert\" % \"0.2.5\" % \"test\",",
                "      \"org.mockito\" % \"mockito-core\" % \"4.6.1\" % \"test\",",
                "      \"org.mockito\" % \"mockito-inline\" % \"4.6.1\" % \"test\",",
                "      \"org.mockito\" % \"mockito-junit-jupiter\" % \"3.12.4\" % \"test\",",
                "      \"org.junit.jupiter\" % \"junit-jupiter-api\" % \"5.9.0\" % \"test\",",
                "      \"org.junit.jupiter\" % \"junit-jupiter-engine\" % \"5.9.0\" % \"test\",",
                "      \"com.typesafe.play\" %% \"play-json\" % \"2.9.2\" % \"test\",",
                "      \"com.google.truth\" % \"truth\" % \"1.1.5\" % \"test\",",
                "      \"net.aichler\" % \"jupiter-interface\" % \"0.11.1\" % Test",
                "    ),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    publish / skip := true)",
                "",
                "",
                "lazy val flintCommons = (project in file(\"flint-commons\"))",
                "  .settings(",
                "    commonSettings,",
                "    name := \"flint-commons\",",
                "    scalaVersion := scala212,",
                "    libraryDependencies ++= Seq(",
                "      \"org.scalactic\" %% \"scalactic\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest-flatspec\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatestplus\" %% \"mockito-4-6\" % \"3.2.15.0\" % \"test\",",
                "      \"org.projectlombok\" % \"lombok\" % \"1.18.30\" % \"provided\",",
                "    ),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    publish / skip := true,",
                "    assembly / test := (Test / test).value,",
                "    assembly / assemblyOption ~= {",
                "      _.withIncludeScala(false)",
                "    },",
                "    assembly / assemblyMergeStrategy := {",
                "      case PathList(ps@_*) if ps.last endsWith (\"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case PathList(\"module-info.class\") => MergeStrategy.discard",
                "      case PathList(\"META-INF\", \"versions\", xs@_, \"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case x =>",
                "        val oldStrategy = (assembly / assemblyMergeStrategy).value",
                "        oldStrategy(x)",
                "    },",
                "  )",
                "  .enablePlugins(AssemblyPlugin)",
                "",
                "",
                "lazy val pplSparkIntegration = (project in file(\"ppl-spark-integration\"))",
                "  .enablePlugins(AssemblyPlugin, Antlr4Plugin)",
                "  .settings(",
                "    commonSettings,",
                "    name := \"ppl-spark-integration\",",
                "    scalaVersion := scala212,",
                "    libraryDependencies ++= Seq(",
                "      \"org.scalactic\" %% \"scalactic\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest-flatspec\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatestplus\" %% \"mockito-4-6\" % \"3.2.15.0\" % \"test\",",
                "      \"com.stephenn\" %% \"scalatest-json-jsonassert\" % \"0.2.5\" % \"test\",",
                "      \"com.github.sbt\" % \"junit-interface\" % \"0.13.3\" % \"test\",",
                "      \"org.projectlombok\" % \"lombok\" % \"1.18.30\",",
                "      \"com.github.seancfoley\" % \"ipaddress\" % \"5.5.1\",",
                "    ),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    // ANTLR settings",
                "    Antlr4 / antlr4Version := \"4.8\",",
                "    Antlr4 / antlr4PackageName := Some(\"org.opensearch.flint.spark.ppl\"),",
                "    Antlr4 / antlr4GenListener := true,",
                "    Antlr4 / antlr4GenVisitor := true,",
                "    // Assembly settings",
                "    assemblyPackageScala / assembleArtifact := false,",
                "    assembly / assemblyOption ~= {",
                "      _.withIncludeScala(false)",
                "    },",
                "    assembly / assemblyMergeStrategy := {",
                "      case PathList(ps @ _*) if ps.last endsWith (\"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case PathList(\"module-info.class\") => MergeStrategy.discard",
                "      case PathList(\"META-INF\", \"versions\", xs @ _, \"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case x =>",
                "        val oldStrategy = (assembly / assemblyMergeStrategy).value",
                "        oldStrategy(x)",
                "    },",
                "    assembly / test := (Test / test).value)",
                "",
                "lazy val flintSparkIntegration = (project in file(\"flint-spark-integration\"))",
                "  .dependsOn(flintCore, flintCommons)",
                "  .enablePlugins(AssemblyPlugin, Antlr4Plugin)",
                "  .settings(",
                "    commonSettings,",
                "    name := \"flint-spark-integration\",",
                "    scalaVersion := scala212,",
                "    libraryDependencies ++= Seq(",
                "      \"com.amazonaws\" % \"aws-java-sdk\" % \"1.12.397\" % \"provided\"",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      \"org.scalactic\" %% \"scalactic\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatest\" %% \"scalatest-flatspec\" % \"3.2.15\" % \"test\",",
                "      \"org.scalatestplus\" %% \"mockito-4-6\" % \"3.2.15.0\" % \"test\",",
                "      \"org.mockito\" % \"mockito-inline\" % \"4.6.0\" % \"test\",",
                "      \"com.stephenn\" %% \"scalatest-json-jsonassert\" % \"0.2.5\" % \"test\",",
                "      \"com.github.sbt\" % \"junit-interface\" % \"0.13.3\" % \"test\"),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    // ANTLR settings",
                "    Antlr4 / antlr4Version := \"4.8\",",
                "    Antlr4 / antlr4PackageName := Some(\"org.opensearch.flint.spark.sql\"),",
                "    Antlr4 / antlr4GenListener := true,",
                "    Antlr4 / antlr4GenVisitor := true,",
                "    // Assembly settings",
                "    assemblyPackageScala / assembleArtifact := false,",
                "    assembly / assemblyOption ~= {",
                "      _.withIncludeScala(false)",
                "    },",
                "    assembly / assemblyMergeStrategy := {",
                "      case PathList(ps @ _*) if ps.last endsWith (\"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case PathList(\"module-info.class\") => MergeStrategy.discard",
                "      case PathList(\"META-INF\", \"versions\", xs @ _, \"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case x =>",
                "        val oldStrategy = (assembly / assemblyMergeStrategy).value",
                "        oldStrategy(x)",
                "    },",
                "    assembly / test := (Test / test).value)",
                "",
                "lazy val IntegrationTest = config(\"it\") extend Test",
                "lazy val AwsIntegrationTest = config(\"aws-it\") extend Test",
                "",
                "// Test assembly package with integration test.",
                "lazy val integtest = (project in file(\"integ-test\"))",
                "  .dependsOn(flintCommons % \"test->test\", flintSparkIntegration % \"test->test\", pplSparkIntegration % \"test->test\", sparkSqlApplication % \"test->test\")",
                "  .configs(IntegrationTest, AwsIntegrationTest)",
                "  .settings(",
                "    commonSettings,",
                "    name := \"integ-test\",",
                "    scalaVersion := scala212,",
                "    javaOptions ++= Seq(",
                "      s\"-DappJar=${(sparkSqlApplication / assembly).value.getAbsolutePath}\",",
                "      s\"-DextensionJar=${(flintSparkIntegration / assembly).value.getAbsolutePath}\",",
                "      s\"-DpplJar=${(pplSparkIntegration / assembly).value.getAbsolutePath}\",",
                "    ),",
                "    inConfig(IntegrationTest)(Defaults.testSettings ++ Seq(",
                "      IntegrationTest / javaSource := baseDirectory.value / \"src/integration/java\",",
                "      IntegrationTest / scalaSource := baseDirectory.value / \"src/integration/scala\",",
                "      IntegrationTest / resourceDirectory := baseDirectory.value / \"src/integration/resources\",",
                "        IntegrationTest / parallelExecution := false,",
                "      IntegrationTest / fork := true,",
                "    )),",
                "    inConfig(AwsIntegrationTest)(Defaults.testSettings ++ Seq(",
                "      AwsIntegrationTest / javaSource := baseDirectory.value / \"src/aws-integration/java\",",
                "      AwsIntegrationTest / scalaSource := baseDirectory.value / \"src/aws-integration/scala\",",
                "      AwsIntegrationTest / parallelExecution := false,",
                "      AwsIntegrationTest / fork := true,",
                "    )),",
                "    libraryDependencies ++= Seq(",
                "      \"com.amazonaws\" % \"aws-java-sdk\" % \"1.12.397\" % \"provided\"",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      \"org.scalactic\" %% \"scalactic\" % \"3.2.15\",",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"com.stephenn\" %% \"scalatest-json-jsonassert\" % \"0.2.5\" % \"test\",",
                "      \"org.testcontainers\" % \"testcontainers\" % \"1.18.0\" % \"test\",",
                "      \"org.apache.iceberg\" %% s\"iceberg-spark-runtime-$sparkMinorVersion\" % icebergVersion % \"test\",",
                "      \"org.scala-lang.modules\" %% \"scala-collection-compat\" % \"2.11.0\" % \"test\"),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    Test / fullClasspath ++= Seq((flintSparkIntegration / assembly).value, (pplSparkIntegration / assembly).value,",
                "      (sparkSqlApplication / assembly).value",
                "    ),",
                "    IntegrationTest / dependencyClasspath ++= (Test / dependencyClasspath).value,",
                "    AwsIntegrationTest / dependencyClasspath ++= (Test / dependencyClasspath).value,",
                "    integration := (IntegrationTest / test).value,",
                "    awsIntegration := (AwsIntegrationTest / test).value",
                "  )",
                "lazy val integration = taskKey[Unit](\"Run integration tests\")",
                "lazy val awsIntegration = taskKey[Unit](\"Run AWS integration tests\")",
                "",
                "lazy val standaloneCosmetic = project",
                "  .settings(",
                "    name := \"opensearch-spark-standalone\",",
                "    commonSettings,",
                "    releaseSettings,",
                "    exportJars := true,",
                "    Compile / packageBin := (flintSparkIntegration / assembly).value)",
                "",
                "lazy val sparkSqlApplication = (project in file(\"spark-sql-application\"))",
                "  // dependency will be provided at runtime, so it doesn't need to be included in the assembled JAR",
                "  .dependsOn(flintSparkIntegration % \"provided\")",
                "  .settings(",
                "    commonSettings,",
                "    name := \"sql-job\",",
                "    scalaVersion := scala212,",
                "    libraryDependencies ++= Seq(",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\"),",
                "    libraryDependencies ++= deps(sparkVersion),",
                "    libraryDependencies ++= Seq(",
                "      \"com.typesafe.play\" %% \"play-json\" % \"2.9.2\",",
                "      \"com.amazonaws\" % \"aws-java-sdk-glue\" % \"1.12.568\" % \"provided\"",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      // handle AmazonS3Exception",
                "      \"com.amazonaws\" % \"aws-java-sdk-s3\" % \"1.12.568\" % \"provided\"",
                "        // the transitive jackson.core dependency conflicts with existing scala",
                "        // error: Scala module 2.13.4 requires Jackson Databind version >= 2.13.0 and < 2.14.0 -",
                "        // Found jackson-databind version 2.14.2",
                "        exclude (\"com.fasterxml.jackson.core\", \"jackson-databind\"),",
                "      \"org.scalatest\" %% \"scalatest\" % \"3.2.15\" % \"test\",",
                "      \"org.mockito\" %% \"mockito-scala\" % \"1.16.42\" % \"test\",",
                "      \"org.scalatestplus\" %% \"mockito-4-6\" % \"3.2.15.0\" % \"test\"),",
                "    // Assembly settings",
                "    // the sbt assembly plugin found multiple copies of the module-info.class file with",
                "    // different contents in the jars  that it was merging flintCore dependencies.",
                "    // This can happen if you have multiple dependencies that include the same library,",
                "    // but with different versions.",
                "    assemblyPackageScala / assembleArtifact := false,",
                "    assembly / assemblyOption ~= {",
                "      _.withIncludeScala(false)",
                "    },",
                "    assembly / assemblyMergeStrategy := {",
                "      case PathList(ps@_*) if ps.last endsWith (\"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case PathList(\"module-info.class\") => MergeStrategy.discard",
                "      case PathList(\"META-INF\", \"versions\", xs@_, \"module-info.class\") =>",
                "        MergeStrategy.discard",
                "      case x =>",
                "        val oldStrategy = (assembly / assemblyMergeStrategy).value",
                "        oldStrategy(x)",
                "    },",
                "    assembly / test := (Test / test).value",
                "  )",
                "",
                "lazy val sparkSqlApplicationCosmetic = project",
                "  .settings(",
                "    name := \"opensearch-spark-sql-application\",",
                "    commonSettings,",
                "    releaseSettings,",
                "    exportJars := true,",
                "    Compile / packageBin := (sparkSqlApplication / assembly).value)",
                "",
                "lazy val sparkPPLCosmetic = project",
                "  .settings(",
                "    name := \"opensearch-spark-ppl\",",
                "    commonSettings,",
                "    releaseSettings,",
                "    exportJars := true,",
                "    Compile / packageBin := (pplSparkIntegration / assembly).value)",
                "",
                "lazy val releaseSettings = Seq(",
                "  publishMavenStyle := true,",
                "  publishArtifact := true,",
                "  Test / publishArtifact := false,",
                "  licenses += (\"Apache-2.0\", url(\"http://www.apache.org/licenses/LICENSE-2.0\")),",
                "  pomExtra :=",
                "    <url>https://opensearch.org/</url>",
                "      <scm>",
                "        <url>git@github.com:opensearch-project/opensearch-spark.git</url>",
                "        <connection>scm:git:git@github.com:opensearch-project/opensearch-spark.git</connection>",
                "      </scm>)",
                ""
            ]
        },
        {
            "file_name": "scalastyle-config.xml",
            "comments": [
                "<!--",
                "  ~ Licensed to the Apache Software Foundation (ASF) under one or more",
                "  ~ contributor license agreements.  See the NOTICE file distributed with",
                "  ~ this work for additional information regarding copyright ownership.",
                "  ~ The ASF licenses this file to You under the Apache License, Version 2.0",
                "  ~ (the \"License\"); you may not use this file except in compliance with",
                "  ~ the License.  You may obtain a copy of the License at",
                "  ~",
                "  ~    http://www.apache.org/licenses/LICENSE-2.0",
                "  ~",
                "  ~ Unless required by applicable law or agreed to in writing, software",
                "  ~ distributed under the License is distributed on an \"AS IS\" BASIS,",
                "  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "  ~ See the License for the specific language governing permissions and",
                "  ~ limitations under the License.",
                "  -->",
                "<!--",
                "",
                "If you wish to turn off checking for a section of code, you can put a comment in the source",
                "before and after the section, with the following syntax:",
                "",
                "  // scalastyle:off",
                "  ...  // stuff that breaks the styles",
                "  // scalastyle:on",
                "",
                "You can also disable only one rule, by specifying its rule id, as specified in:",
                "  http://www.scalastyle.org/rules-0.7.0.html",
                "",
                "  // scalastyle:off no.finalize",
                "  override def finalize(): Unit = ...",
                "  // scalastyle:on no.finalize",
                "",
                "This file is divided into 3 sections:",
                " (1) rules that we enforce.",
                " (2) rules that we would like to enforce, but haven't cleaned up the codebase to turn on yet",
                "     (or we need to make the scalastyle rule more configurable).",
                " (3) rules that we don't want to enforce.",
                "-->",
                "",
                "<scalastyle>",
                "    <name>Scalastyle standard configuration</name>",
                "",
                "    <!-- ================================================================================ -->",
                "    <!--                               rules we enforce                                   -->",
                "    <!-- ================================================================================ -->",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.FileTabChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.HeaderMatchesChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"header\"><![CDATA[/*",
                " * Copyright OpenSearch Contributors",
                " * SPDX-License-Identifier: Apache-2.0",
                " */]]></parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.SpacesAfterPlusChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.SpacesBeforePlusChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.WhitespaceEndOfLineChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.FileLineLengthChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"maxLineLength\"><![CDATA[200]]></parameter>",
                "            <parameter name=\"tabSize\"><![CDATA[2]]></parameter>",
                "            <parameter name=\"ignoreImports\">true</parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.ClassNamesChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\"><![CDATA[[A-Z][A-Za-z]*]]></parameter></parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.ObjectNamesChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\"><![CDATA[(config|[A-Z][A-Za-z]*)]]></parameter></parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.PackageObjectNamesChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\"><![CDATA[^[a-z][A-Za-z]*$]]></parameter></parameters>",
                "    </check>",
                "",
                "    <check customId=\"argcount\" level=\"error\" class=\"org.scalastyle.scalariform.ParameterNumberChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"maxParameters\"><![CDATA[10]]></parameter></parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NoFinalizeChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.CovariantEqualsChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.StructuralTypeChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.UppercaseLChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.IfBraceChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"singleLineAllowed\"><![CDATA[true]]></parameter>",
                "            <parameter name=\"doubleLineAllowed\"><![CDATA[true]]></parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.PublicMethodsHaveTypeChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.NewLineAtEofChecker\" enabled=\"true\"></check>",
                "",
                "    <check customId=\"nonascii\" level=\"error\" class=\"org.scalastyle.scalariform.NonASCIICharacterChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.SpaceAfterCommentStartChecker\" enabled=\"false\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.EnsureSingleSpaceBeforeTokenChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"tokens\">ARROW, EQUALS, ELSE, TRY, CATCH, FINALLY, LARROW, RARROW</parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.EnsureSingleSpaceAfterTokenChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"tokens\">ARROW, EQUALS, COMMA, COLON, IF, ELSE, DO, WHILE, FOR, MATCH, TRY, CATCH, FINALLY, LARROW, RARROW</parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <!-- ??? usually shouldn't be checked into the code base. -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NotImplementedErrorUsage\" enabled=\"true\"></check>",
                "",
                "    <!-- As of SPARK-7558, all tests in Spark should extend o.a.s.SparkFunSuite instead of AnyFunSuite directly -->",
                "    <check customId=\"funsuite\" level=\"error\" class=\"org.scalastyle.scalariform.TokenChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">^AnyFunSuite[A-Za-z]*$</parameter></parameters>",
                "        <customMessage>Tests must extend org.apache.spark.SparkFunSuite instead.</customMessage>",
                "    </check>",
                "",
                "    <!-- As of SPARK-7977 all printlns need to be wrapped in '// scalastyle:off/on println' -->",
                "    <check customId=\"println\" level=\"error\" class=\"org.scalastyle.scalariform.TokenChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">^println$</parameter></parameters>",
                "        <customMessage><![CDATA[Are you sure you want to println? If yes, wrap the code block with",
                "      // scalastyle:off println",
                "      println(...)",
                "      // scalastyle:on println]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"hadoopconfiguration\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">spark(.sqlContext)?.sparkContext.hadoopConfiguration</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use sparkContext.hadoopConfiguration? In most cases, you should use",
                "      spark.sessionState.newHadoopConf() instead, so that the hadoop configurations specified in Spark session",
                "      configuration will come into effect.",
                "      If you must use sparkContext.hadoopConfiguration, wrap the code block with",
                "      // scalastyle:off hadoopconfiguration",
                "      spark.sparkContext.hadoopConfiguration...",
                "      // scalastyle:on hadoopconfiguration",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"visiblefortesting\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">@VisibleForTesting</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      @VisibleForTesting causes classpath issues. Please note this in the java doc instead (SPARK-11615).",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"runtimeaddshutdownhook\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Runtime\\.getRuntime\\.addShutdownHook</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use Runtime.getRuntime.addShutdownHook? In most cases, you should use",
                "      ShutdownHookManager.addShutdownHook instead.",
                "      If you must use Runtime.getRuntime.addShutdownHook, wrap the code block with",
                "      // scalastyle:off runtimeaddshutdownhook",
                "      Runtime.getRuntime.addShutdownHook(...)",
                "      // scalastyle:on runtimeaddshutdownhook",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"mutablesynchronizedbuffer\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">mutable\\.SynchronizedBuffer</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use mutable.SynchronizedBuffer? In most cases, you should use",
                "      java.util.concurrent.ConcurrentLinkedQueue instead.",
                "      If you must use mutable.SynchronizedBuffer, wrap the code block with",
                "      // scalastyle:off mutablesynchronizedbuffer",
                "      mutable.SynchronizedBuffer[...]",
                "      // scalastyle:on mutablesynchronizedbuffer",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"classforname\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Class\\.forName</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use Class.forName? In most cases, you should use Utils.classForName instead.",
                "      If you must use Class.forName, wrap the code block with",
                "      // scalastyle:off classforname",
                "      Class.forName(...)",
                "      // scalastyle:on classforname",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"awaitresult\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Await\\.result</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use Await.result? In most cases, you should use ThreadUtils.awaitResult instead.",
                "      If you must use Await.result, wrap the code block with",
                "      // scalastyle:off awaitresult",
                "      Await.result(...)",
                "      // scalastyle:on awaitresult",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"awaitready\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Await\\.ready</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use Await.ready? In most cases, you should use ThreadUtils.awaitReady instead.",
                "      If you must use Await.ready, wrap the code block with",
                "      // scalastyle:off awaitready",
                "      Await.ready(...)",
                "      // scalastyle:on awaitready",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"caselocale\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">(\\.toUpperCase|\\.toLowerCase)(?!(\\(|\\(Locale.ROOT\\)))</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use toUpperCase or toLowerCase without the root locale? In most cases, you",
                "      should use toUpperCase(Locale.ROOT) or toLowerCase(Locale.ROOT) instead.",
                "      If you must use toUpperCase or toLowerCase without the root locale, wrap the code block with",
                "      // scalastyle:off caselocale",
                "      .toUpperCase",
                "      .toLowerCase",
                "      // scalastyle:on caselocale",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"throwerror\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">throw new \\w+Error\\(</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to throw Error? In most cases, you should use appropriate Exception instead.",
                "      If you must throw Error, wrap the code block with",
                "      // scalastyle:off throwerror",
                "      throw new XXXError(...)",
                "      // scalastyle:on throwerror",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <!-- As of SPARK-9613 JavaConversions should be replaced with JavaConverters -->",
                "    <check customId=\"javaconversions\" level=\"error\" class=\"org.scalastyle.scalariform.TokenChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">JavaConversions</parameter></parameters>",
                "        <customMessage>Instead of importing implicits in scala.collection.JavaConversions._, import",
                "            scala.collection.JavaConverters._ and use .asScala / .asJava methods</customMessage>",
                "    </check>",
                "",
                "    <check customId=\"commonslang2\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">org\\.apache\\.commons\\.lang\\.</parameter></parameters>",
                "        <customMessage>Use Commons Lang 3 classes (package org.apache.commons.lang3.*) instead",
                "            of Commons Lang 2 (package org.apache.commons.lang.*)</customMessage>",
                "    </check>",
                "",
                "    <check customId=\"executioncontextglobal\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">scala\\.concurrent\\.ExecutionContext\\.Implicits\\.global</parameter></parameters>",
                "        <customMessage> User queries can use global thread pool, causing starvation and eventual OOM.",
                "            Thus, Spark-internal APIs should not use this thread pool</customMessage>",
                "    </check>",
                "",
                "    <check customId=\"FileSystemGet\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">FileSystem.get\\([a-zA-Z_$][a-zA-Z_$0-9]*\\)</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that you want to use \"FileSystem.get(Configuration conf)\"? If the input",
                "      configuration is not set properly, a default FileSystem instance will be returned. It can",
                "      lead to errors when you deal with multiple file systems. Please consider using",
                "      \"FileSystem.get(URI uri, Configuration conf)\" or \"Path.getFileSystem(Configuration conf)\" instead.",
                "      If you must use the method \"FileSystem.get(Configuration conf)\", wrap the code block with",
                "      // scalastyle:off FileSystemGet",
                "      FileSystem.get(...)",
                "      // scalastyle:on FileSystemGet",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"extractopt\" level=\"error\" class=\"org.scalastyle.scalariform.TokenChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">extractOpt</parameter></parameters>",
                "        <customMessage>Use jsonOption(x).map(.extract[T]) instead of .extractOpt[T], as the latter",
                "            is slower.  </customMessage>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.ImportOrderChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"groups\">java,scala,3rdParty,spark</parameter>",
                "            <parameter name=\"group.java\">javax?\\..*</parameter>",
                "            <parameter name=\"group.scala\">scala\\..*</parameter>",
                "            <parameter name=\"group.3rdParty\">(?!org\\.apache\\.spark\\.).*</parameter>",
                "            <parameter name=\"group.spark\">org\\.apache\\.spark\\..*</parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.DisallowSpaceBeforeTokenChecker\" enabled=\"true\">",
                "        <parameters>",
                "            <parameter name=\"tokens\">COMMA</parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <!-- SPARK-3854: Single Space between ')' and '{' -->",
                "    <check customId=\"SingleSpaceBetweenRParenAndLCurlyBrace\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">\\)\\{</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Single Space between ')' and `{`.",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check customId=\"NoScalaDoc\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">(?m)^(\\s*)/[*][*].*$(\\r|)\\n^\\1  [*]</parameter></parameters>",
                "        <customMessage>Use Javadoc style indentation for multiline comments</customMessage>",
                "    </check>",
                "",
                "    <check customId=\"OmitBracesInCase\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">case[^\\n>]*=>\\s*\\{</parameter></parameters>",
                "        <customMessage>Omit braces in case clauses.</customMessage>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">new (java\\.lang\\.)?(Byte|Integer|Long|Short)\\(</parameter></parameters>",
                "        <customMessage>Use static factory 'valueOf' or 'parseXXX' instead of the deprecated constructors.</customMessage>",
                "    </check>",
                "",
                "    <!-- SPARK-16877: Avoid Java annotations -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.OverrideJavaChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.DeprecatedJavaChecker\" enabled=\"true\"></check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.IllegalImportsChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"illegalImports\"><![CDATA[scala.collection.Seq,scala.collection.IndexedSeq]]></parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Don't import scala.collection.Seq and scala.collection.IndexedSeq as it may bring some problems with cross-build between Scala 2.12 and 2.13.",
                "",
                "      Please refer below page to see the details of changes around Seq / IndexedSeq.",
                "      https://docs.scala-lang.org/overviews/core/collections-migration-213.html",
                "",
                "      If you really need to use scala.collection.Seq or scala.collection.IndexedSeq, please use the fully-qualified name instead.",
                "    ]]></customMessage>",
                "    </check>",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.IllegalImportsChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"illegalImports\"><![CDATA[org.apache.log4j]]></parameter></parameters>",
                "        <customMessage>Please use Apache Log4j 2 instead.</customMessage>",
                "    </check>",
                "",
                "",
                "    <!-- ================================================================================ -->",
                "    <!--       rules we'd like to enforce, but haven't cleaned up the codebase yet        -->",
                "    <!-- ================================================================================ -->",
                "",
                "    <!-- We cannot turn the following two on, because it'd fail a lot of string interpolation use cases. -->",
                "    <!-- Ideally the following two rules should be configurable to rule out string interpolation. -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NoWhitespaceBeforeLeftBracketChecker\" enabled=\"false\"></check>",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NoWhitespaceAfterLeftBracketChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- This breaks symbolic method names so we don't turn it on. -->",
                "    <!-- Maybe we should update it to allow basic symbolic names, and then we are good to go. -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.MethodNamesChecker\" enabled=\"false\">",
                "        <parameters>",
                "            <parameter name=\"regex\"><![CDATA[^[a-z][A-Za-z0-9]*$]]></parameter>",
                "        </parameters>",
                "    </check>",
                "",
                "    <!-- Should turn this on, but we have a few places that need to be fixed first -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.EqualsHashCodeChecker\" enabled=\"true\"></check>",
                "",
                "    <!-- ================================================================================ -->",
                "    <!--                               rules we don't want                                -->",
                "    <!-- ================================================================================ -->",
                "",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.IllegalImportsChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"illegalImports\"><![CDATA[sun._,java.awt._]]></parameter></parameters>",
                "    </check>",
                "",
                "    <!-- We want the opposite of this: NewLineAtEofChecker -->",
                "    <check level=\"error\" class=\"org.scalastyle.file.NoNewLineAtEofChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- This one complains about all kinds of random things. Disable. -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.SimplifyBooleanExpressionChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- We use return quite a bit for control flows and guards -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.ReturnChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- We use null a lot in low level code and to interface with 3rd party code -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NullChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- Doesn't seem super big deal here ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NoCloneChecker\" enabled=\"false\"></check>",
                "",
                "    <!-- Doesn't seem super big deal here ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.file.FileLengthChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"maxFileLength\">800></parameter></parameters>",
                "    </check>",
                "",
                "    <!-- Doesn't seem super big deal here ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NumberOfTypesChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"maxTypes\">30</parameter></parameters>",
                "    </check>",
                "",
                "    <!-- Doesn't seem super big deal here ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"maximum\">10</parameter></parameters>",
                "    </check>",
                "",
                "    <!-- Doesn't seem super big deal here ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.MethodLengthChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"maxLength\">50</parameter></parameters>",
                "    </check>",
                "",
                "    <!-- Not exactly feasible to enforce this right now. -->",
                "    <!-- It is also infrequent that somebody introduces a new class with a lot of methods. -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.NumberOfMethodsInTypeChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"maxMethods\"><![CDATA[30]]></parameter></parameters>",
                "    </check>",
                "",
                "    <!-- Doesn't seem super big deal here, and we have a lot of magic numbers ... -->",
                "    <check level=\"error\" class=\"org.scalastyle.scalariform.MagicNumberChecker\" enabled=\"false\">",
                "        <parameters><parameter name=\"ignore\">-1,0,1,2,3</parameter></parameters>",
                "    </check>",
                "",
                "    <check customId=\"GuavaToStringHelper\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Objects.toStringHelper</parameter></parameters>",
                "        <customMessage>Avoid using Object.toStringHelper. Use ToStringBuilder instead.</customMessage>",
                "    </check>",
                "",
                "    <check customId=\"GuavaFilesCreateTempDir\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">Files\\.createTempDir\\(</parameter></parameters>",
                "        <customMessage>Avoid using com.google.common.io.Files.createTempDir due to CVE-2020-8908.",
                "            Use org.apache.spark.util.Utils.createTempDir instead.",
                "        </customMessage>",
                "    </check>",
                "",
                "    <check customId=\"pathfromuri\" level=\"error\" class=\"org.scalastyle.file.RegexChecker\" enabled=\"true\">",
                "        <parameters><parameter name=\"regex\">new Path\\(new URI\\(</parameter></parameters>",
                "        <customMessage><![CDATA[",
                "      Are you sure that this string is uri encoded? Please be careful when converting hadoop Paths",
                "      and URIs to and from String. If possible, please use SparkPath.",
                "    ]]></customMessage>",
                "    </check>",
                "</scalastyle>",
                ""
            ]
        }
    ]
}